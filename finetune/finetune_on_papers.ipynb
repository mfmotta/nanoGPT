{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from pathlib import Path\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"papers\"\n",
    "arxiv_ids = [2210.03162, 2206.07682, 2306.08055]  #, 1904.07094, 2203.00759, 2104.08691, 2212.01681]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_file_path, abstract_file_path = create_corpus(dir_path, arxiv_ids)\n",
    "text_file_path, abstract_file_path ='data/papers_text.txt', 'data/papers_abstract.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset(text_file_path, abstract_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models : Introduction Language models (LMs), such as GPT-2 (Radford et al., 2018, 2019a), BERT (Devlin et al., 2018), T5 (Raffel et al., 2020), or GPT-3 (Brown et al., 2020), exhibit a remarkable ability to capture pat- terns of grammar, vocabulary, cultural knowledge, and conversational rhythms present in natural lan- guage. Formally, a LM is a conditional distribution over tokens p(xt|x1, · · · , xt−1), with each token xt ∈ V for some vocabulary V. Throughout this paper, we will refer to xh = x1, · · · , xt−1 as the prompt. This paper explores prompt compression: the idea that the text xh used to condition a LM can be approximately represented by a much smaller set of carefully chosen weights, using the framework of soft prompts (Lester et al., 2021). We begin by establishing some basic properties of compressed prompts, and importantly show that while highly ∗ Work done while at Nvidia, Inc. † Work done while at Brigham Young University Figure 1: Schematic of prompt compression. Weights of the soft prompt are tuned to minimize the KL diver- gence between hard and soft prompts, for all xt:k. compressed prompts lose ﬁne-grained information about the prompt, they can retain general, abstract information. This motivates our central application: to use such compressed prompts in a Bayesian at- tribute framework to steer text generation, with speciﬁc application to toxicity reduction. To motivate this more deeply, we brieﬂy sketch how compressed prompts can be used in toxicity reduction. Efforts to reduce toxicity and bias gener- ally follow one of two strategies: the ﬁrst is to train or ﬁne-tune LMs on carefully curated data, either tagging or labelling it in special ways (Keskar et al., 2019a; Lu et al., 2022) or using data known to be “clean”. The second is to \"steer\" the generation of token probabilities away from toxic generations (Krause et al., 2020; Liu et al., 2021), and towards text with known, desirable properties. Following previous work, we steer LM prob- abilities by using a Bayesian attribute classiﬁer framework that involves scoring candidate tokens with different experts. As an independent contri- bution, we explore the idea of simply using condi- tioning text to construct such experts by leveraging the few-shot modeling abilities of LMs (Radford et al., 2019a; Brown et al., 2020): given a few examples of text containing a pattern of interest, language models are capable of “analyzing” such examples and assign high probability to subsequent text exhibiting the same pattern. Thus, in the same way that language model can, for example, clas- LMLMKLHard promptSoft prompt  sify the sentiment of a tweet, we use LMs to ana- lyze the toxicity of candidate generations in real- time. Our method can be considered an exemplar- based method of deﬁning experts that capture de- sirable and undesirable attributes of generated text. We term this technique contrastive contexts, and note that it reduces the problem of creating experts to one of prompt engineering (Reynolds and Mc- Donell, 2021). However, our conditioning contexts are quite large, which motivated this work. We use prompt compression to mimic an uncompressed prompt (hereafter referred to as \"hard\" prompt) as closely as possible, thereby saving both computation and space in the context window. Our results demon- strate that this can be very effective, and, in a very surprising ﬁnding, that complex prompts can be reduced to a single token and still be useful for tox- icity reduction, often with better ﬂuency compared to hard prompts. The contributions of this paper are three-fold: ﬁrst, we introduce and formalize the idea of prompt compression; second, we introduce and formalize the method of contrastive contexts in the Bayesian attribute framework; third, we experimentally eval- uate our methods, and reﬁne the technique based on various empirical observations, and contribute a careful study of effectiveness as model size varies. 2 Background and Related Work To the best of our knowledge, this is the ﬁrst work to directly explore prompt compression. However, our work is based on the original soft prompt ideas of (Lester et al., 2021). It is also somewhat related to distillation, where one model is trained to mimic another by matching logits (Gou et al., 2021). Usually, LMs take text as input, which is then to- kenized into discrete tokens by a tokenizer. Each to- ken is then mapped to a learned embedding, which is used as input to a transformer (Vaswani et al., 2017). The idea of soft prompts (Lester et al., 2021) is to bypass the need to use discrete tokens with pre-trained embeddings and instead directly learn a series of embeddings via backpropagation. These learned embeddings are then fed directly to the transformer and do not need to correspond to any actual language tokens. As the centerpiece application of prompt com- pression, we explore generative controllability (Keskar et al., 2019b) and toxicity reduction in language models. Figure 2: KL divergence of compressed prompts as a function of number of tokens n. Prompts are ran- domly sampled from the Pile (mean words= 916, me- dian words = 274, median characters = 1849). Our method is most closely related to decode- time algorithms, such as GEDI (Krause et al., 2020), which uses Bayes’ rule and discriminative models to steer the generation towards a certain attribute; and PPLM (Dathathri et al., 2019), which uses an estimated gradient with respect to the de- sired attribute to steer the LM’s internal representa- tion at generation time. Other methods are based on ﬁne-tuning language models with the classical language modeling ob- jective to steer generation. DEXPERTs (Liu et al., 2021) combines experts and anti-experts in a prod- uct of experts model to reduce toxicity of LMs. Additionally, reinforcement learning approaches show strong performance at steering language mod- els (Stiennon et al., 2020). By providing rewards, methods such as PPO (Schulman et al., 2017) and Quark (Lu et al., 2022) represent the current best performance at reducing LM toxicity while main- taining ﬂuency. These methods, however, require a predetermined reward function, which may or may not be feasible depending on the context. 3 Prompt Compression Here, we introduce and explore the idea of prompt compression, whereby the parameters of a soft prompt (Lester et al., 2021) are trained to mimic a ﬁxed hard prompt as closely as possible. The intuition of our idea is simple: condition- ing a LM on a hard prompt xh induces a distri- bution p(xt, · · · , xt+k|xh) over all possible sub- sequent sequences of tokens xt, · · · , xt+k for all k. To simplify notation, let xt:k = xt, · · · , xt+k. The schematic of the idea is shown in Fig. 1. For- Number of tokens in compressed promptExpected KL divergence mally, a soft prompt is a block of weights θn that is prepended to the embeddings of the tokenized sequence xt:k, and which is then fed through the transformer layers of the language model. The soft prompt induces a modiﬁed distribution over xt:k, which we represent as q(xt:k|θn). Here, n is the number of tokens in the soft prompt (which do not necessarily correspond to natural language tokens). To compress prompt xh, we train the soft prompt weights to minimize the following objective: Ext:k [KL(p(xt:k|xh)||q(xt:k|θn))] where the sequences xt:k’s are sentences of various lengths and content drawn from a diverse training set. We optimize this objective using the Adam optimizer for 75,000 steps of training with a linear learning rate schedule starting at 0.1, and xt:k’s drawn randomly from The Pile (Gao et al., 2021), requiring about 1-4 GPU-hours to train a single prompt, depending on computational complexity of running the LM. All prompt training was done using either a single A100 or V100 GPU. While training a compressed prompt can be ex- pensive, the gains are found at inference time. Us- ing a compressed prompt over a hard prompt re- duces the length of the context. This scales down the needed computation according to the trans- former’s attention mechanism, which is O(n2). This also could allow long contexts to be com- pressed and appended to longer inputs than was pre- viously possible. Once trained, these compressed prompts could be shared to create a library of efﬁ- cient contexts. 4 Experiment Set #1: Establishing Basic Properties of Compressed Prompts We begin by exploring the properties of com- pressed prompts. First, we show that condition- ing on a hard prompt and its compressed prompt generate qualitatively similar generations, although this equivalence degrades as the prompt is com- pressed more and more. Second, we qualitatively explore what happens to ﬁne-grained information as a prompt is compressed more and more. Models and codebase. All experiments were conducted using the Huggingface1 (Wolf et al., 2019) implementation of GPT-2 (117M parame- ters), GPT-2 medium (345M), GPT-2 large (774M) and GPT-2 xl (1.5B) models. 1https://github.com/huggingface/transformers Figure 3: Reading comprehension performance by question as context is more and more compressed. Ac- curacy is averaged over 1000 completions and each line represents a single question. As expected, performance degrades nearly monotonically as the number of tokens in the compressed prompt is decreased. General ques- tions degrade less than questions about speciﬁc details. We used GPT-2 xl for this experiment. 4.1 Comparing hard and compressed prompts Fig. 2 shows the KL divergence between the orig- inal prompt and the compressed prompts’ output distribution for randomly sampled sentences from the pile (Gao et al., 2021). As the ﬁgure shows, as the size of the compressed prompt increases, the KL divergence monotonically decreases for all models. This implies, as expected, that the more context allowed in a soft prompt, the better the soft prompt does at mimicking the full context. Additionally, note that the magnitude of the KL divergence is similar across models for a given soft prompt size n. This shows that this method of context compression works well on a variety of model sizes (124M - 1.5B parameters). 4.2 Exploring information retention As a prompt is compressed more and more, infor- mation in the original prompt must be lost. As the training process speciﬁcally attempts to match the predictive distribution over completions for a prompt, the question arises: what information is preserved, and what is discarded? Reading Comprehension Task. To assess this, we construct the following experiment: given a reading comprehension task that involves a para- graph p of text and a series of questions, how do the answers to those questions degrade as a function of compression? Speciﬁcally, we look at questions about ﬁne-grained information (speciﬁc details that Number of tokens in compressed promptQA Accuracy Figure 4: Assessing the information retained as a prompt is compressed more and more severely. The model is tasked with recovering the passage given a hard prompt (the passage), compressed prompts, or no prompt. For each token, likelihood is calculated and scaled so that the probability according to the hard context is 1 and the probability with no context is 0. It is visualized with a heatmap, where yellow corresponds to 1 (hard context) and pink corresponds to 0 (no context). occur once) in p, as well questions about general information (common themes of the passage that occur multiple times) in p. For the paragraph p and questions used, see Appendix B. In Fig. 3, we see that prompt compression at- tempts to retain the more general information about a prompt, even while it cannot retain ﬁne-grained details. Additionally, as we would expect, the more compression happens, the more information from p is lost. In the next section, we will see that this property will be useful in the context of toxicity reduction. Reconstruction Task. Another way to measure the information retained is to task the language model with reconstructing the paragraph. Specif- ically, given a soft context of a certain length, we append the prompt \"Now repeat the text:\". We then look at the likelihood of each token in the text, normalized between the baselines of no compres- sion (\"hard\" context) and no context at all (note that some words may be easily predicted simply by virtue of grammatical rules of English). Results are shown in Fig. 4. For the heatmap over the full paragraph, see Appendix D. As expected, as n decreases, so does the amount of retained information about the paragraph p. The largest soft prompt (n = 64) seems to re- tain information primarily about the following to- kens: \"Frank and Cindy\", \"bakers\", \"city of Paris, France\", \"They love travelling\", \"visited\", \"coun- tries,\" \"cruises,\" etc. At the lowest size of n = 1, most of the information is lost, but the model still predicts \"Frank\", \"of\", and \"France\" with signiﬁ- cantly higher probability than having no context. Qualitatively, it also appears that, at least for this prompt, information earlier in the prompt is re- tained better than information later in the prompt. 4.3 Compositionality of compressed prompts Here, we brieﬂy explore the idea that multiple severely compressed prompts can be combined to modulate different properties of generated text. To do this, we use two contexts: one that primes the LM for negative sentiment, and a second that primes the model for talking about cats (both con- texts can be found in Appendix A). We then test the effect of steering the model towards different types of text by conditioning on a context which is either negative, talks about cats, or both. In this experiment, we prompt the model with \"I thought the movie was,\" trying to prompt the model to output a movie review. As you can see in Table 1, when you use none of the contexts for conditioning, the baseline level of prompts gener- ated that are about cats or with negative sentiment is low. As you condition on the (normal or soft) contexts individually, the number of completions which contain negativity and cats respectively in- crease. This shows the efﬁcacy of the in context style transfer for both attributes, using either soft or \"hard\" prompts. Finally, when you concatenate the two contexts, you see behavior somewhere in be- tween the baseline and the individual completions. This shows that to some degree, you can compose these soft prompts together to steer completion be- havior. Interestingly, the prompts that best elicit neg- ative sentiment and sentences about cats are the compressed prompt versions. This suggests that the compressed prompt may capture the essence of the preceding text better than the \"hard\" prompt, and may therefore be better for steerability. One potential hypothesis for why the com- pressed prompts may work better than hard prompts in this case is that the prompt has to distill as much information as possible in the prompt, and the most common piece of information is \"cats\" or \"negativity\". Thus, the compressed prompt could contain a more distilled version of the important part of the prompt, leading to strong performance. baseline no prompt hard prompts neg. 0.92 cats 0.34 neg.+cats 0.65 cats neg. sentiment compressed prompts (n = 32) neg. 0.94 neg.+cats 0.23 0.76 cats 0.69 0.31 Table 1: Given the prompt, \"I thought the movie was,\" various preconditioning methods are applied and composed. Sampled completions are then rated for negativity and whether or not they contain the word \"cat\". Numbers are percentage of samples with the intended property over 100 generations. Figure 5: Contrastive conditioning. Content warning: The example text is offensive. A given context is evaluated three times; the positive and negative probabilities are token-wise normalized, combined with the prior probabili- ties, and then globally normalized. 5 The Bayesian Attribute Classiﬁer Framework As an application of compressed prompts, we now turn our attention to toxicity reduction and control- lability. Following previous work (Dathathri et al., 2019; Krause et al., 2020) we adopt the Bayesian attribute classiﬁer framework for decode-time con- trollability. Our goal is to generate text that exhibits some attribute a; by conditioning generations on this attribute and using Bayes law, we arrive at p(xt|a, xh) ∝ p(a|xh, xt)ωp(xt|xh) where the prior p(xt|xh) is simply the vanilla dis- tribution over generations from the LM, and the likelihood term p(a|x1, · · · , xt)ω is known as an attribute classiﬁer. Here, we have also introduced the temperature parameter ω that modulates the strength of the effect of the attribute classiﬁer. There are multiple ways to construct the attribute classiﬁer p(a|xh, xt). If the desired attribute a is, for example, “does not contain profanity”, then the attribute classiﬁer could simply scan xh for words on a blacklist and output p(a|xh, xt) = 1 if none of the words are present. More sophisticated approaches are possible; our approach centers on the careful construction of this classiﬁer. Our work is most similar technically to the GEDI framework (Krause et al., 2020), which uses two language models to construct the classi- ﬁer in a contrastive manner. However, the GEDI framework requires multiple auxiliary language models trained to generate text according to some distribution. Here, we replace those auxiliary lan- guage models with carefully constructed contexts exhibiting the desired attributes for use with a sin- gle language model. 5.1 Constructing experts via contrastive contexts Our approach leverages the few-shot modeling ca- pabilities of language models to deﬁne the attribute classiﬁer. Speciﬁcally,we deﬁne the attribute clas- siﬁer as: p(a|xh, xt) ≡ p(xt|x+ (cid:12) xh) p(xt|x+ (cid:12) xh) + p(xt|x− (cid:12) xh) where the term p(xt|x+ (cid:12) xh) is the probability of xt given xh concatenated with an additional con- text, x+. We term this the positive context. The term p(xt|x− (cid:12) xh) is likewise constructed by con- catenating xh with a negative context, x−. By constructing multiple auxiliary contexts, we use the language model’s inherent ability to ana- lyze text as a way to steer content, resulting in a natural, exemplar-based framework. Our method Birthdays are so fun! Everyone loves beaches. The food in Spain is excellent. Kittens are ﬂuﬀy and lovely. The party wasThepartywasPositive contextNegative contextPoor people don’t deserve nice things. Women should stay in the home. You’re an idiot. Go to $%#!. The party wasamazingawfulcinematicokay...............amazingawfulcinematicokayOriginal contextxPositive probabilitiesFinal probabilitiesAttribute probabilitiesPrior probabilitiesNegative probabilities0.090.010.000.050.120.010.020.100.010.100.020.030.920.090.500.770.420.000.010.11 Figure 6: Toxicity reduction using hard contexts, for various settings of the ω parameter and various model sizes. Smaller models experience a stronger effect. provides state-of-the-art decoder-time detoxiﬁca- tion and requires no backprop through the model, ﬁne-tuning, or carefully curated datasets. It is com- putationally efﬁcient, and can be easily extended to both encourage and inhibit speciﬁc properties of the generated text. Fig. 5 shows the overall ﬂow of the algorithm. For each token generated, the LM is run three times: once to compute p(xt|xh) (which we term the prior probability), once to compute p(xt|x+ (cid:12) xh) (which we term the positive probability), and once to compute p(xt|x− (cid:12) xh) (which we term the neg- ative probability). These three probabilities are then combined according to Equations 2 and 3 to form the ﬁnal token distribution, which can then be used with standard generation methods (such as beam search, nucleus sampling (Holtzman et al., 2020), etc.). Evaluation of the positive and negative proba- bilities involve combining the current history xh with a positive or negative context. In all of our experiments, we simply concatenate them together, although more sophisticated combination strategies are possible. 5.2 Toxicity reduction As an application for prompt compression, we fo- cus on the problem of toxicity reduction. Language models generate text consistent with their training corpus; while it is exciting to see LMs exhibit state of the art performance on a wide variety of natural language tasks, such as text summarization (Raf- fel et al., 2020), conversation (Adiwardana et al., 2020; Zhang et al., 2019), text generation (Rad- ford et al., 2019b; Dai et al., 2019; Keskar et al., 2019a), and zero-shot learning (Brown et al., 2020; Krause et al., 2020), it is equally concerning to see them reﬂect racial bias, gender stereotypes, harm- ful rhetoric, and political misinformation. Unchecked reliance on data-driven algorithmic decision-making can entrench racial, gender, and economic inequalities (Garg et al., 2018; Caliskan et al., 2017; Barocas and Selbst, 2016; Mayson, 2018; Panch et al., 2019; Obermeyer et al., 2019; Lazer et al., 2020). As a result, the machine learn- ing community is rightfully concerned with reduc- ing toxicity and bias. We leverage the method of contrastive contexts to address the problem of toxicity reduction. While the prompts x+ and x− could in principle contain a variety of different types of text, for the remainder of this paper we restrict our attention to the case where we wish to inhibit profane, vulgar, sexist, and racist text (although see Sec. 4.2 for an exam- ple of more general usage). For toxicity reduction, the positive and negative contexts can be consid- ered exemplars in a few-shot modeling framework: the positive context literally contains sentences that are polite in content and tone, while the negative context contains a variety of snippets of racist, sex- ist, and vulgar sentences. Intuitively, then, our method reduces toxicity by asking: is the token that is about to be generated, when combined with xh, more similar in tone and content to the exemplar sentences in the positive or the negative contexts? The contrast between the token likelihood in these two contexts yields the ﬁnal attribute classiﬁer. The contexts used are listed in Appendix A. The experiments discussed in Sec. 6 show that contrastive conditioning can be an effective method for toxicity reduction. However, it comes with a 0510Strength of effect (omega)0.20.40.60.8Expected max toxicityToxic prompts  Non-toxic prompts  Toxic prompts  Non-toxic promptsgpt2gpt2-mediumgpt2-largegpt2-xlPPLM baseline Figure 7: Toxicity reduction using compressed prompts, for various settings of the ω parameter, various model sizes, and various amounts of compression. Surprisingly, more compression leads to better toxicity reduction, and complex prompts can be compressed to a single soft token. to thoroughly capture the wide variety of cost: ways to be toxic, the contexts we use are quite large (for example, our standard toxic context is around 900 tokens). This introduces two problems: ﬁrst, the toxic context ﬁlls up most of the con- text window available to standard models (often 1024 tokens), and second, incurs signiﬁcant com- putational burden, and motivates application of our prompt compression technique. 6 Experiment Set #2: Application to Toxicity Reduction To empirically assess prompt compression in the context of toxicity reduction, we follow the ex- perimental protocol outlined in the RealToxici- tyPrompts (RTP) paper (Gehman et al., 2020). The RTP paper contributes both a dataset and a vari- ety of metrics for assessing toxicity; we brieﬂy summarize those here. Also following the RTP paper, all toxicity measurements are done with the PerspectiveAPI (Jigsaw and the Google Counter Abuse Team, 2015), an imperfect (Sap et al., 2019) but standard tool for assessing toxicity along a va- riety of dimensions. The RTP paper contributes a dataset of 100,000 prompts balanced across different levels of toxicity. For each prompt, a LM is tasked with generating 25 continuations; each continuation is then analyzed for toxic content. There are two primary metrics of interest. The ﬁrst is the expected maximum tox- icity, where the max toxicity is taken over the 25 generations, and second is the average toxicity. 6.1 Experimental setup Construction of contexts. The contrastive condi- tioning technique requires a toxic prompt, and a positive prompt. The toxic prompt was constructed by hand by manually assembling a variety of racist, sexist, prejudiced, profane and vulgar text (the full context can be accessed from Appendix A.1). Spelling, capitalization and grammar were varied to avoid creating unwanted patterns in the prompt. We lightly optimized the creation of the prompt, testing only three variants and settling on the longest and most diverse contexts. It is possible that the way these prompts describe toxicity is not well aligned with the Perspective API; more tight alignment with downstream evaluators is an area for future research. The positive context was constructed sim- ilarly, and is listed in Appendix A.3. RTP prompts. For computational reasons, ex- periments were done on a ﬁxed subset of 2000 randomly sampled RTP prompts, resulting in a bal- anced set of toxic and non-toxic prompts. 6.2 Toxicity reduction with hard prompts We begin by evaluating toxicity reduction using contrastive conditioning with hard prompts, as de- scribed in Sec. 5. We followed the RTP protocol as closely as possible: for each prompt in our RTP subset, we generated 25 completions, each con- sisting of (up to) 20 tokens. Completions were then scored using the Perspective API, and we then calculated both the Expected Max Toxicity and Average Toxicity metrics. We tested all four language models, across a variety of settings for the ω hyperparameter. Our results are shown in Fig. 6. As ω increases, toxi- city reduction is increased. At its highest setting, our method produces results competitive with the SOTA decoding method at the time of writing, which is PPLM. We also note that our technique 0510Strength of effect ()0.30.40.50.6Expected max toxicityGPT-20510GPT-2 Medium0510GPT-2 Large0510GPT-2 XL1248163264Hard produces a weaker effect on larger models. This is consistent with observations made in other papers (Liu et al., 2021), although it has not been system- atically explored. Some example generations can be found in Appendix C. 6.3 Toxicity reduction with soft prompts As noted in Sec. 3, there are multiple disadvantages to the large contexts we used in the previous section. Here we explore the use of compressed prompts in the context of toxicity reduction. We compress both toxic and positive contexts, and then run the same suite of toxicity reduction experiments as described in the previous section. The results are summarized in Fig. 7. The ﬁgure shows reduction as a function of ω, for a variety of models and a variety of lengths n. There are several noteworthy results: ﬁrst, basically all com- pressed prompts perform at least as well as their corresponding hard prompt; second, as noted pre- viously, larger models show a weaker effect than smaller models; and third, soft prompts as small as a single token often provide the best effects. (The original toxic prompt is around 900 tokens long, so this represents a 900x compression rate). This surprising result is not well understood. While severely compressed prompts do not convey the entire richness of the original prompt (as ex- plored in Sec. 4.1), they apparently provide enough contrast that they can be used in the Bayesian at- tribute classiﬁer framework. perplexity, the soft contexts generally achieve a lower expected max toxicity. In addition, the small- est soft contexts (n = 1, 2, 4) often achieve the lowest expected max toxicity without additional loss to ﬂuency. While this behavior is not well understood, we hypothesize that the smallest com- pressed prompts learn only the essential attribute of the contexts (toxicity) and can better steer gener- ations. 6.5 Steering large models with smaller ones Multiple other authors have noted that large mod- els can be steered with experts derived from small models, with good results and reduced computa- tion (Liu et al., 2021; Krause et al., 2020). Here, we systematically explore this idea in the context of contrastive conditioning, comparing both hard prompts and soft prompts. We test the entire ma- trix of using each GPT-2 model to steer each other model, including testing cases where small models are steered by large models. The results are shown in Fig. 9. On the left, base models (rows) are steered by different models (columns). We report expected maximum toxicity. See that in every case, toxicity is reduced most when steered by the smallest models, a result in line with prior work (Liu et al., 2021). The same pattern holds for an equivalent experiment using compressed prompts, as shown on the right panel. We set ω = 10 and n = 64; qualitatively similar results are obtained with other values. 6.4 Trade-off with ﬂuency 7 Conclusions and Future Work For each model, we sweep several values of ω over the soft and hard prompts and compare expected max toxicity with perplexity, a surrogate for ﬂuency. Perplexity is measured with respect to a larger lan- guage model, GPT-J (6 billion parameters).2 Results are shown in Figure 8. In general, there is a trade-off between expected max toxicity and ﬂuency. By strategically selecting ω, one can opti- mize the amount of ﬂuency sacriﬁced for toxicity reduction. This trade-off is expected as the lan- guage model must sacriﬁce its original objective (perplexity) for the steering objective (controlla- bility); this is line with previous work (Liu et al., 2021; Lu et al., 2022). Interestingly enough, the soft prompts scale simi- larly to or better than the hard prompts. For a given 2We calculate perplexity using 3000 sampled completions for each hyperparameters combination. We have explored the idea of prompt compres- sion, establishing basic properties of the method and then examining an extended application to controllability and toxicity reduction. Based on our experiments, we conclude that prompts can be signiﬁcantly compressed and still retain some useful information. As an analogy, severely com- pressed prompts seem to retain a \"semantic eigen- vector\" that summarizes the aspects of a prompt that have the largest effect on downstream token sequences. This suggests that representing infor- mation as basic tokenized sentences is inefﬁcient, and that more general prompt compression strate- gies may be possible (for example, by training a prompt-compressing deep neural network). We see that compressed prompts generally exhibit the properties we would expect them to, such as natu- rally retaining the most important information as Figure 8: Trade-off between Expected max toxicity and ﬂuency. Perplexity is measured according to GPT-J (6B). Controllability strength (ω) values are shown for the hard contexts and follow the same pattern for the soft contexts. Soft contexts generally get lower toxicity given a ﬁxed perplexity value. timately, however, the possibilities and limitations of the method are an open question. 8 Acknowledgements This material is based upon work supported by the National Science Foundation under Grant No. 2141680. Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reﬂect the views of the National Science Foundation. We also gratefully acknowledge Bryan Catan- zaro, Nvidia, Inc., and the Applied Deep Learn- ing Research team for their support and helpful insights. 9 Limitations The method of prompt compression has a variety of limitations. Here, we summarize a few of the most noteworthy. Computational limitations: compressing a sin- gle prompt currently requires several hours of com- pute on state-of-the-art hardware. This is reminis- cent of other DNN-based optimization problems, such as early work in style transfer; an intriguing possibility is to, like in the style transfer litera- ture, train a generic prompt compressor that would quickly compress any prompt. Theoretical limitations: we have sketched a va- riety of properties and applications for compressed prompts, but there is currently no theoretical char- acterization of the method. Formally measuring, for example, the information content in a com- pressed prompt relative to subsequent token se- quences is likely possible and useful. Application limitations: our method yields state-of-the-art toxicity reduction for decode-time Figure 9: Steering large LMs with smaller LMs, with both hard and soft prompts. Color represents Expected Max toxicity, with ω = 10.0 (and n = 64 for com- pressed prompts). In every case, we ﬁnd that small models do a better job of steering large models. they are compressed further and further. We have also sketched some initial results show- ing that compressed prompts can be used for con- trollability, but there is much more work to be done along these lines. While we have shown that our method is effective at general toxicity reduction, it is less likely to be effective at reducing (for exam- ple) general bias, such as subtle sexism, without more advanced prompt engineering methods. Finally, while computationally expensive to cre- ate, compressed prompts may be useful in situa- tions where the same prompt is used again and again, because compressed prompts require less compute at inference time. Additionally, they may allow more information to be included in the con- text window of a language model by composing multiple compressed prompts together, or mix- ing and matching compressed prompts with hard prompts. In this way, information from contexts that would ordinarily be too long to include in the contexts at the same time could be combined. Ul- 406080Perplexity ( is more fluent)0.30.40.50.6Expected max toxicity = 0 = 1 = 2 = 3 = 5 = 10GPT-230405060 = 0 = 1 = 2 = 3 = 5 = 10GPT-2 Medium20304050 = 0 = 1 = 2 = 3 = 5 = 10GPT-2 Large20304050 = 0 = 1 = 2 = 3 = 5 = 10GPT-2 XL1248163264HardSMLXLSteering modelSMLXLBase model0.320.380.390.40.330.380.40.410.330.390.410.430.340.390.40.42Hard promptsSMLXLSMLXL0.310.350.370.380.320.360.380.40.330.380.390.410.330.380.390.42Compressed prompts methods. Even so, toxicity is not reduced to zero, and therefore this method should not be deployed in production systems with zero tolerance for toxic generations. In addition, other types of methods (e.g., Quark (Lu et al., 2022)) provide better over- all toxicity reduction, at the cost of modifying the LM’s weights; in cases where modifying the weights is possible (and no dynamic changes are needed, or where no compositionality of multi- ple steering directions is needed), those methods should be preferred. 10 Ethics While we hope that our work can enable positive downstream applications, such as toxicity reduc- tion, we realize that the method can be trivially applied to increase toxicity or any other undesir- able characteristic. However, we do not feel that our controllability method fundamentally goes far beyond currently available controllability applica- tions so there is little additional risk. That being said, we urge any people who use our method to be conscientious and ethical about applications. Additionally, as noted earlier, our method may not be able to reduce all kinds of toxicity, especially when it comes to subtler toxicity (sexism, microag- gressions, etc.) Further research is needed to make toxicity detection and mitigation more robust. References Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. 2020. Towards a human-like open-domain chatbot. arXiv preprint arXiv:2001.09977. Solon Barocas and Andrew D. Selbst. 2016. Big Data’s Disparate Impact. California Law Review, 104:671. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv:2005.14165. Aylin Caliskan, and Arvind Joanna J Bryson, Narayanan. 2017. Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334):183–186. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car- bonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language mod- els beyond a ﬁxed-length context. arXiv preprint arXiv:1901.02860. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2019. Plug and play language mod- els: a simple approach to controlled text generation. arXiv preprint arXiv:1912.02164. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805. Leo Gao, Stella Biderman, Sid Black, Laurence Gold- ing, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2021. The pile: An 800gb dataset of diverse text for language modeling. Nikhil Garg, Londa Schiebinger, Dan Jurafsky, and James Zou. 2018. Word embeddings quantify 100 years of gender and ethnic stereotypes. Pro- ceedings of the National Academy of Sciences, 115(16):E3635–E3644. Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. Realtoxici- typrompts: Evaluating neural toxic degeneration in language models. ArXiv, abs/2009.11462. Jianping Gou, Baosheng Yu, Stephen J. Maybank, and Dacheng Tao. 2021. Knowledge distillation: A International Journal of Computer Vision, survey. 129(6):1789–1819. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. Jigsaw and the Google Counter Abuse Team. 2015. [link]. Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. 2019a. Ctrl: A conditional transformer language model for control- lable generation. arXiv preprint arXiv:1909.05858. Nitish Shirish Keskar, Bryan McCann, Lav R. Varsh- ney, Caiming Xiong, and Richard Socher. 2019b. Ctrl: A conditional transformer language model for controllable generation. Ben Krause, Akhilesh Deepak Gotmare, Bryan Mc- Cann, Nitish Shirish Keskar, Shaﬁq Joty, Richard Socher, and Nazneen Fatema Rajani. 2020. Gedi: Generative discriminator guided sequence genera- tion. arXiv preprint arXiv:2009.06367. David M. Lazer, Alex Pentland, Duncan J. Watts, Sinan Aral, Susan Athey, Nosihir Contractor, Deen Freelon, Sandra Gonzalez-Bailon, Gary King, He- len Margetts, Alondra Nelson, Matthew J. Sal- ganik, Markus Strohmaier, Alessandro Vespignani, and Claudia Wagner. 2020. Computational so- cial science: Obstacles and opportunities. Science, 369(6507):1060–1062. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efﬁcient prompt tuning. Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin Choi. 2021. Dexperts: Decoding- time controlled text generation with experts and anti- experts. Ximing Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. 2022. Quark: Controllable text gen- eration with reinforced unlearning. Sandra G. Mayson. 2018. Bias in, Bias out. Yale Law Journal, 128:2218. Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. Dissecting racial bias in an algorithm used to manage the health of popula- tions. Science, 366(6464):447–453. Trishan Panch, Heather Mattie, and Rifat Atun. 2019. Artiﬁcial intelligence and algorithmic bias: Implica- tions for health systems. Journal of Global Health, 9(2):010318. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing with unsupervised learning. Technical re- port, OpenAI. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019a. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019b. Lan- guage models are unsupervised multitask learners. OpenAI Blog, 1(8):9. Colin Raffel, Noam Shazeer, Adam Roberts, Kather- ine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a uniﬁed text-to- text transformer. Journal of Machine Learning Re- search, 21(140):1–67. Laria Reynolds and Kyle McDonell. 2021. Prompt pro- gramming for large language models: Beyond the few-shot paradigm. Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A. Smith. 2019. The risk of racial bias In Proceedings of the in hate speech detection. 57th Annual Meeting of the Association for Com- putational Linguistics, pages 1668–1678, Florence, Italy. Association for Computational Linguistics. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. Learning to summarize from human feedback. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all In Advances in neural information pro- you need. cessing systems, pages 5998–6008. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow- icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2019. Huggingface’s transformers: State-of-the-art natural language processing. Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2019. Dialogpt: Large-scale generative pre-training for conversational response generation. arXiv preprint arXiv:1911.00536. A Contexts used The toxic contexts and negative contexts used in- clude highly offensive material. To avoid gratu- itously including offensive text, we have decided to omit these contexts from the paper. With a desire for transparency and replicability, we are making them available at https://github.com/BYU- PCCL/prompt-compression-contrastive-coding. A.1 Toxic Context This content was omitted to avoid the inclusion of gratuitous offensive material. The interested reader may ﬁnd it at https://github.com/BYU- PCCL/prompt-compression-contrastive-coding. A.2 Negative Context This content was omitted to avoid the inclusion of gratuitous offensive material. The interested reader may ﬁnd it at https://github.com/BYU- PCCL/prompt-compression-contrastive-coding. A.3 Positive Context These are examples of sentences that are kind, generous, polite, uplifting, noble and that express positive sentiments: Everyone loves kittens. The food and drink of Spain are both outstanding. I think everyone deserves a second chance. longsuffering We need to be committed to loving other people, no matter what. Love never fails. You are one of my nicest thoughts. Decide that no matter what is happening around you, the sun will always be shining in your world. Birthdays, rainbows, gentle rainfall, puppies and hot chocolate by the ﬁre are some of my favorite things. grace Patience is a virtue. The only way to ﬁnd peace in our time is if each individual lives unselﬁshly, caring for their neighbor more than themselves. Gentleness While I appreciate your opinion on this matter, I think it could have been said more generously. Only kindness matters. FAITH A sweet friendship refreshes the soul. I am so thankful for my parents and for my teachers. They’ve made a positive difference in my life! If we’re going to overcome the perils of identity politics, we all need to listen to each other, and really try to understand their point of view. Who doesn’t love going to the beach? Families are forever Giving to charity is one of the noblest things a person can do. Friendship is the icing on the cake of life. beauty Reach out to the poor, the downtrodden and the suffering, and you will ﬁnd eternal life. Dancing and singing lets the soul roam free. Independence is happiness. Humanity is our race; love is our religion. You can’t rely on how you look to sustain you, what sustains us, what is fundamentally beautiful is compassion; for yourself and your those around you. Count your blessings! Peace & love, baby. A.4 Cat Context These are all sentences about cats: Cats are the best! I REALLY LOVE CATS. Did you know that the Egyptians worshipped cats? Cats are by far the internet’s most popular animal. It’s true that cats can be independent and aloof, but they are also loyal and compassionate. the poor animal was beginning to think \"bad cat\" was her new name The cat is a popular pet animal which wass tamed by humans a long time ago. Cats are friendly and playful with people, espe- cially with children. The product is applied to a cat daily and reduces dander from the coat, which can cause allergic reactions. Cats have four legs and one tail and they produce a “meow”, “purr” and “hiss” sound. I thought I might just as well describe my pet in order to know it–order, vertebrate; division, quadruped; class, mammalia; genus, felinus; species, cat; individual, Tabby. Laser pointers are probably one of the most engaging ways to play with a cat. Catnip really does act like a mild stimulant for cats. Once I was surprised to see a cat walking along the stony shore of the pond, for they rarely wander so far from home. The cat can have some milk, and the mouse can have some cake. Joseph asked as he waved a foot at the cat, who scurried back and repeated her greeting. he giggled and cuddled the cat clos Jane said I have to leave the cat with you. FleaScan helps you identify ﬂea infestation in any dog or cat long before becoming full-blown. B Reading Comprehension Experiment Details B.1 Paragraph Frank and Cindy are bakers in the city of Paris, France. They love traveling, and have visited nu- merous countries around the world. They enjoy cruises, hiking, and visiting cities with history and ﬂair. Because they are bakers, they also enjoy ex- ploring new foods, tasting new wine, and interact- ing with local cooks and chefs. Frank and Cindy travel 2-3 times per year, and have visited Europe, South America and Australia. They have not vis- ited Africa, but hope to someday. They also enjoy posting stories about their travels on Facebook and trying to convince their friends to travel with them. Answer: \"They also enjoy posting stories about their travels • Question: Complete the following sentence from the paragraph about Frank and Cindy: \"They love...\" Answer: \"They love traveling C Example Generations Here are some example generations for the prompt \"Its editor has written that black people are\" from RTP. All generations below are steered with hard prompts and the Bayesian attribute classiﬁer with ω = 10 and τ = 1. • Vanilla GPT-2: Its editor has written that black people are white and they need a \"real understanding\" of feminism. • Steered GPT-2: Its editor has written that black people are often misunderstood — many wrongly. • Steered GPT-2 medium: Its editor has written that black people are often judged on their race or color, sometimes negatively. Ross has offered some insight into his own journey • Steered GPT-2 large: Its editor has written that black people are rarely portrayed kind in traditional music, particularly in traditional African music — and that blackness tends to • Steered GPT-2 xl: Its editor has written that black people are particularly concerned about Donald Trump. D Full Reading Comprehension Heatmap B.2 Speciﬁc Questions Below we put the questions in black, and the an- swers in red. • Question: What profession is Frank? Answer: Frank is a baker • Question: What continent have Frank and Cindy not visited? Answer: They have not visited Africa • Question: Question: How often do Frank and Cindy travel? Answer: They travel 2-3 times per year • Question: Where do Frank and Cindy post stories about their travel? Answer: They post on Facebook • Question: Complete the following sentence from the paragraph about Frank and Cindy: \"Frank and Cindy are bakers in the city of...\" Answer: \"Frank and Cindy are bakers in the city of Paris • Question: Complete the following sentence from the paragraph about Frank and Cindy: \"they also enjoy exploring new...\" Answer: \"they also enjoy exploring new cities • Question: Complete the following sentence from the paragraph about Frank and Cindy: \"they also enjoy exploring new foods, tasting new...\" Answer: \"they also enjoy exploring new foods, tasting new\" foods • Question: Complete the following sentence from the paragraph about Frank and Cindy: \"they also enjoy exploring new foods, tasting new...\" Answer: \"they also enjoy exploring new foods, tasting new\" wine • Question: Complete the following sentence from the paragraph about Frank and Cindy: \"tasting new wine, and interacting with lo- cal...\" Answer: \"tasting new wine, and inter- acting with local cooks B.3 Generic Questions • Question: What do Frank and Cindy love to do? Answer: They love to travel • Question: Complete the following sentence from the paragraph about Frank and Cindy: \"They also enjoy posting stories about their...\" Figure 10: Full Heatmap assessment of information retained as a prompt is compressed more and more severely. hardFrank and Cindy are bakers in the city of Paris, France. They love traveling, and have visited numerous countries around the world. They enjoy cruises, hiking, and visiting citieswith history and flair. Because they are bakers, they also enjoy exploring new foods, tasting new wine, and interacting with local cooks and chefs. Frank and Cindy travel 2-3 timesper year, and have visited Europe, South America and Australia. They have not visited Africa, but hope to someday. They also enjoy posting stories about their travels on Facebookand trying to convince their friends to travel with them.soft_dim_64Frank and Cindy are bakers in the city of Paris, France. They love traveling, and have visited numerous countries around the world. They enjoy cruises, hiking, and visiting citieswith history and flair. Because they are bakers, they also enjoy exploring new foods, tasting new wine, and interacting with local cooks and chefs. Frank and Cindy travel 2-3 timesper year, and have visited Europe, South America and Australia. They have not visited Africa, but hope to someday. They also enjoy posting stories about their travels on Facebookand trying to convince their friends to travel with them.soft_dim_32Frank and Cindy are bakers in the city of Paris, France. They love traveling, and have visited numerous countries around the world. They enjoy cruises, hiking, and visiting citieswith history and flair. Because they are bakers, they also enjoy exploring new foods, tasting new wine, and interacting with local cooks and chefs. Frank and Cindy travel 2-3 timesper year, and have visited Europe, South America and Australia. They have not visited Africa, but hope to someday. They also enjoy posting stories about their travels on Facebookand trying to convince their friends to travel with them.soft_dim_16Frank and Cindy are bakers in the city of Paris, France. They love traveling, and have visited numerous countries around the world. They enjoy cruises, hiking, and visiting citieswith history and flair. Because they are bakers, they also enjoy exploring new foods, tasting new wine, and interacting with local cooks and chefs. Frank and Cindy travel 2-3 timesper year, and have visited Europe, South America and Australia. They have not visited Africa, but hope to someday. They also enjoy posting stories about their travels on Facebookand trying to convince their friends to travel with them.soft_dim_8Frank and Cindy are bakers in the city of Paris, France. They love traveling, and have visited numerous countries around the world. They enjoy cruises, hiking, and visiting citieswith history and flair. Because they are bakers, they also enjoy exploring new foods, tasting new wine, and interacting with local cooks and chefs. Frank and Cindy travel 2-3 timesper year, and have visited Europe, South America and Australia. They have not visited Africa, but hope to someday. They also enjoy posting stories about their travels on Facebookand trying to convince their friends to travel with them.soft_dim_4Frank and Cindy are bakers in the city of Paris, France. They love traveling, and have visited numerous countries around the world. They enjoy cruises, hiking, and visiting citieswith history and flair. Because they are bakers, they also enjoy exploring new foods, tasting new wine, and interacting with local cooks and chefs. Frank and Cindy travel 2-3 timesper year, and have visited Europe, South America and Australia. They have not visited Africa, but hope to someday. They also enjoy posting stories about their travels on Facebookand trying to convince their friends to travel with them.soft_dim_2Frank and Cindy are bakers in the city of Paris, France. They love traveling, and have visited numerous countries around the world. They enjoy cruises, hiking, and visiting citieswith history and flair. Because they are bakers, they also enjoy exploring new foods, tasting new wine, and interacting with local cooks and chefs. Frank and Cindy travel 2-3 timesper year, and have visited Europe, South America and Australia. They have not visited Africa, but hope to someday. They also enjoy posting stories about their travels on Facebookand trying to convince their friends to travel with them.soft_dim_1Frank and Cindy are bakers in the city of Paris, France. They love traveling, and have visited numerous countries around the world. They enjoy cruises, hiking, and visiting citieswith history and flair. Because they are bakers, they also enjoy exploring new foods, tasting new wine, and interacting with local cooks and chefs. Frank and Cindy travel 2-3 timesper year, and have visited Europe, South America and Australia. They have not visited Africa, but hope to someday. They also enjoy posting stories about their travels on Facebookand trying to convince their friends to travel with them.vanillaFrank and Cindy are bakers in the city of Paris, France. They love traveling, and have visited numerous countries around the world. They enjoy cruises, hiking, and visiting citieswith history and flair. Because they are bakers, they also enjoy exploring new foods, tasting new wine, and interacting with local cooks and chefs. Frank and Cindy travel 2-3 timesper year, and have visited Europe, South America and Australia. They have not visited Africa, but hope to someday. They also enjoy posting stories about their travels on Facebookand trying to convince their friends to travel with them.',\n",
       " 'Emergent Abilities of Large Language Models : 1 Introduction Language models have revolutionized natural language processing (NLP) in recent years. It is now well-known that increasing the scale of language models (e.g., training compute, model parameters, etc.) can lead to better performance and sample eﬃciency on a range of downstream NLP tasks (Devlin et al., 2019; Brown et al., 2020, inter alia). In many cases, the eﬀect of scale on performance can often be methodologically predicted via scaling laws—for example, scaling curves for cross-entropy loss have been shown to empirically span more than seven orders of magnitude (Kaplan et al., 2020; Hoﬀmann et al., 2022). On the other hand, performance for certain downstream tasks counterintuitively does not appear to continuously improve as a function of scale, and such tasks cannot be predicted ahead of time (Ganguli et al., 2022). In this paper, we will discuss the unpredictable phenomena of emergent abilities of large language models. Emergence as an idea has been long discussed in domains such as physics, biology, and computer science (Anderson, 1972; Hwang et al., 2012; Forrest, 1990; Corradini & O’Connor, 2010; Harper & Lewis, 2012, inter Published in Transactions on Machine Learning Research (08/2022) alia). We will consider the following general deﬁnition of emergence, adapted from Steinhardt (2022) and rooted in a 1972 essay called “More Is Diﬀerent” by Nobel prize-winning physicist Philip Anderson (Anderson, 1972): Emergence is when quantitative changes in a system result in qualitative changes in behavior. Here we will explore emergence with respect to model scale, as measured by training compute and number of model parameters. Speciﬁcally, we deﬁne emergent abilities of large language models as abilities that are not present in smaller-scale models but are present in large-scale models; thus they cannot be predicted by simply extrapolating the performance improvements on smaller-scale models (§2).1 We survey emergent abilities as observed in a range of prior work, categorizing them in settings such as few-shot prompting (§3) and augmented prompting strategies (§4). Emergence motivates future research on why such abilities are acquired and whether more scaling will lead to further emergent abilities, which we highlight as important questions for the ﬁeld (§5). 2 Emergent Abilities Deﬁnition As a broad concept, emergence is often used informally and can be reasonably interpreted in many diﬀerent ways. In this paper, we will consider a focused deﬁnition of emergent abilities of large language models: An ability is emergent if it is not present in smaller models but is present in larger models. Emergent abilities would not have been directly predicted by extrapolating a scaling law (i.e. consistent performance improvements) from small-scale models. When visualized via a scaling curve (x-axis: model scale, y-axis: performance), emergent abilities show a clear pattern—performance is near-random until a certain critical threshold of scale is reached, after which performance increases to substantially above random. This qualitative change is also known as a phase transition—a dramatic change in overall behavior that would not have been foreseen by examining smaller-scale systems (Huberman & Hogg, 1987). Today’s language models have been scaled primarily along three factors: amount of computation, number of model parameters, and training dataset size (Kaplan et al., 2020; Hoﬀmann et al., 2022). In this paper, we will analyze scaling curves by plotting the performance of diﬀerent models where training compute for each model is measured in FLOPs on the x-axis (Hoﬀmann et al., 2022). Because language models trained with more compute tend to also have more parameters, we additionally show plots with number of model parameters as the x-axis in Appendix D (see Figure 11 and Figure 12, as well as Figure 4 and Figure 10). Using training FLOPs or model parameters as the x-axis produces curves with similar shapes due to the fact that most dense Transformer language model families have scaled training compute roughly proportionally with model parameters (Kaplan et al., 2020). Training dataset size is also an important factor, but we do not plot capabilities against it because many language model families use a ﬁxed number of training examples for all model sizes (Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022). Although we focus on training computation and model size here, there is not a single proxy that adequately captures all aspects of scale. For example, Chinchilla (Hoﬀmann et al., 2022) has one-fourth as many parameters as Gopher (Rae et al., 2021) but uses similar training compute; and sparse mixture-of-expert models have more parameters per training/inference compute than dense models (Fedus et al., 2021; Du et al., 2021). Overall, it may be wise to view emergence as a function of many correlated variables. For example, later in Figure 4 we will also plot emergence as a function of WikiText103 perplexity (Merity et al., 2016), which happens to closely correlate with training computation for Gopher/ Chinchilla (though this correlation may not hold in the long-run). Note that the scale at which an ability is ﬁrst observed to emerge depends on a number of factors and is not an immutable property of the ability. For instance, emergence may occur with less training compute 1This survey focuses on pre-trained Transformer language models. Emergent abilities in NLP more broadly, however, could go back to Miller et al. (2004), Liang (2005), or earlier. Published in Transactions on Machine Learning Research (08/2022) or fewer model parameters for models trained on higher-quality data. Conversely, emergent abilities also crucially depend on other factors such as not being limited by the amount of data, its quality, or the number of parameters in the model. Today’s language models are likely not trained optimally (Hoﬀmann et al., 2022), and our understanding of how to best train models will evolve over time. Our goal in this paper is not to characterize or claim that a speciﬁc scale is required to observe emergent abilities, but rather, we aim to discuss examples of emergent behavior in prior work. 3 Few-Shot Prompted Tasks We ﬁrst discuss emergent abilities in the prompting paradigm, as pop- ularized by GPT-3 (Brown et al., 2020).2 In prompting, a pre-trained language model is given a prompt (e.g. a natural language instruction) of a task and completes the response without any further training or gradient updates to its parameters. Brown et al. (2020) proposed few-shot prompting, which includes a few input-output examples in the model’s context (input) as a preamble before asking the model to perform the task for an unseen inference-time example. An example prompt is shown in Figure 1. Figure 1: Example of an input and output for few-shot prompting. The ability to perform a task via few-shot prompting is emergent when a model has random performance until a certain scale, after which performance increases to well-above random. Figure 2 shows eight such emergent abilities spanning ﬁve language model families from various work. BIG-Bench. Figure 2A–D depicts four emergent few-shot prompted tasks from BIG-Bench, a crowd-sourced suite of over 200 benchmarks for language model evaluation (BIG-Bench, 2022). Figure 2A shows an arithmetic benchmark that tests 3-digit addition and subtraction, as well as 2-digit multiplication. GPT-3 and LaMDA (Thoppilan et al., 2022) have close-to-zero performance for several orders of magnitude of training compute, before performance jumps to sharply above random at 2 · 1022 training FLOPs (13B parameters) for GPT-3, and 1023 training FLOPs (68B parameters) for LaMDA. Similar emergent behavior also occurs at around the same model scale for other tasks, such as transliterating from the International Phonetic Alphabet (Figure 2B), recovering a word from its scrambled letters (Figure 2C), and Persian question-answering (Figure 2D). Even more emergent abilities from BIG-Bench are given in Appendix E. TruthfulQA. Figure 2E shows few-shot prompted performance on the TruthfulQA benchmark, which measures the ability to answer questions truthfully (Lin et al., 2021). This benchmark is adversarially curated against GPT-3 models, which do not perform above random, even when scaled to the largest model size. Small Gopher models also do not perform above random until scaled up to the largest model of 5 · 1023 training FLOPs (280B parameters), for which performance jumps to more than 20% above random (Rae et al., 2021). Grounded conceptual mappings. Figure 2F shows the task of grounded conceptual mappings, where language models must learn to map a conceptual domain, such as a cardinal direction, represented in a textual grid world (Patel & Pavlick, 2022). Again, performance only jumps to above random using the largest GPT-3 model. Multi-task language understanding. Figure 2G shows the Massive Multi-task Language Understanding (MMLU) benchmark, which aggregates 57 tests covering a range of topics including math, history, law, and more (Hendrycks et al., 2021a). For GPT-3, Gopher, and Chinchilla, models of ∼1022 training FLOPs (∼10B parameters) or smaller do not perform better than guessing on average over all the topics, scaling up to 3–5 ·1023 training FLOPs (70B–280B parameters) enables performance to substantially surpass random. This result is striking because it could imply that the ability to solve knowledge-based questions spanning a large collection of topics might require scaling up past this threshold (for dense language models without retrieval or access to external memory). 2Though GPT-3 popularized prompting, the task setup has existed since before GPT-3 (Trinh & Le, 2018; McCann et al., 2018; Radford et al., 2019; Raﬀel et al., 2020). Language modelInputOutputReview: This movie sucks.Sentiment: negative.Review: I love this movie.Sentiment:positive. Published in Transactions on Machine Learning Research (08/2022) LaMDA GPT-3 Gopher Chinchilla PaLM Random (A) Mod. arithmetic (B) IPA transliterate (C) Word unscramble (D) Persian QA 1018 1020 1022 1024 1018 1020 1022 1024 1018 1020 1022 1024 1018 1020 1022 1024 (E) TruthfulQA (F) Grounded mappings (G) Multi-task NLU (H) Word in context 1020 1022 1024 1020 1022 1024 1020 1022 1024 1020 1022 1024 Model scale (training FLOPs) Figure 2: Eight examples of emergence in the few-shot prompting setting. Each point is a separate model. The ability to perform a task via few-shot prompting is emergent when a language model achieves random performance until a certain scale, after which performance signiﬁcantly increases to well-above random. Note that models that used more training compute also typically have more parameters—hence, we show an analogous ﬁgure with number of model parameters instead of training FLOPs as the x-axis in Figure 11. A–D: BIG-Bench (2022), 2-shot. E: Lin et al. (2021) and Rae et al. (2021). F: Patel & Pavlick (2022). G: Hendrycks et al. (2021a), Rae et al. (2021), and Hoﬀmann et al. (2022). H: Brown et al. (2020), Hoﬀmann et al. (2022), and Chowdhery et al. (2022) on the WiC benchmark (Pilehvar & Camacho-Collados, 2019). Word in Context. Finally, Figure 2H shows the Word in Context (WiC) benchmark (Pilehvar & Camacho- Collados, 2019), which is a semantic understanding benchmark. Notably, GPT-3 and Chinchilla fail to achieve one-shot performance of better than random, even when scaled to their largest model size of ∼5 · 1023 FLOPs. Although these results so far may suggest that scaling alone may not enable models to solve WiC, above-random performance eventually emerged when PaLM was scaled to 2.5 · 1024 FLOPs (540B parameters), which was much larger than GPT-3 and Chinchilla. 4 Augmented Prompting Strategies Although few-shot prompting is perhaps currently the most common way of interacting with large language models, recent work has proposed several other prompting and ﬁnetuning strategies to further augment the abilities of language models. If a technique shows no improvement or is harmful when compared to the baseline of not using the technique until applied to a model of a large-enough scale, we also consider the technique an emergent ability. Published in Transactions on Machine Learning Research (08/2022) (A) Math word problems (B) Instruction following (C) 8-digit addition Chain of thought No chain of thought Instruction tuning instruction tuning Scratchpad scratchpad 1021 1022 1023 1024 1021 1022 1023 1024 1019 1020 1021 Model scale (training FLOPs) ) (D) Calibration Letter choices 1022 1023 1024 Figure 3: Specialized prompting or ﬁnetuning methods can be emergent in that they do not have a positive eﬀect until a certain model scale. A: Wei et al. (2022b). B: Wei et al. (2022a). C: Nye et al. (2021). D: Kadavath et al. (2022). An analogous ﬁgure with number of parameters on the x-axis instead of training FLOPs is given in Figure 12. The model shown in A-C is LaMDA (Thoppilan et al., 2022), and the model shown in D is from Anthropic. Multi-step reasoning. Reasoning tasks, especially those involving multiple steps, have been challenging for language models and NLP models more broadly (Rae et al., 2021; Bommasani et al., 2021; Nye et al., 2021). A recent prompting strategy called chain-of-thought prompting enables language models to solve such problems by guiding them to produce a sequence of intermediate steps before giving the ﬁnal answer (Cobbe et al., 2021; Wei et al., 2022b; Suzgun et al., 2022). As shown in Figure 3A, chain of thought prompting only surpasses standard prompting without intermediate steps when scaled to 1023 training FLOPs (∼100B parameters). A similar emergence in performance gain was also observed when augmenting few-shot prompting with explanations that came after the ﬁnal answer (Lampinen et al., 2022). Instruction following. Another growing line of work aims to better enable language models to perform new tasks simply by reading instructions describing the task (without few-shot exemplars). By ﬁnetuning on a mixture of tasks phrased as instructions, language models have been shown to respond appropriately to instructions describing an unseen task (Ouyang et al., 2022; Wei et al., 2022a; Sanh et al., 2022; Chung et al., 2022). As shown in Figure 3B, Wei et al. (2022a) found that this instruction-ﬁnetuning technique hurts performance for models of 7 · 1021 training FLOPs (8B parameters) or smaller, and only improves performance when scaled to 1023 training FLOPs (∼100B parameters) (though Sanh et al. (2022) found shortly after that this instruction-following behavior could be also induced by ﬁnetuning smaller encoder-decoder T5 models). Program execution. Consider computational tasks involving multiple steps, such as adding large numbers or executing computer programs. Nye et al. (2021) show that ﬁnetuning language models to predict intermediate outputs (“scratchpad”) enables them to successfully execute such multi-step computations. As shown in Figure 3C, on 8-digit addition, using a scratchpad only helps for models of ∼9 · 1019 training FLOPs (40M parameters) or larger. Model calibration. Finally, an important direction for deployment of language models studies is calibration, which measures whether models can predict which questions they will be able to answer correctly. Kadavath et al. (2022) compared two ways of measuring calibration: a True/False technique, where models ﬁrst propose answers and then evaluate the probability “P(True)” that their answers are correct, and more-standard methods of calibration, which use the probability of the correct answer compared with other answer options. As shown in Figure 3D, the superiority of the True/False technique only emerges when scaled to the largest model scale of ∼3 · 1023 training FLOPs (52B parameters). Published in Transactions on Machine Learning Research (08/2022) Table 1: List of emergent abilities of large language models and the scale (both training FLOPs and number of model parameters) at which the abilities emerge. Emergent scale Train. FLOPs Params. Model Reference Few-shot prompting abilities (cid:114) (cid:114) (cid:114) (cid:114) (cid:114) (cid:114) (cid:114) (cid:114) (cid:114) (cid:114) Addition/subtraction (3 digit) Addition/subtraction (4-5 digit) MMLU Benchmark (57 topic avg.) Toxicity classiﬁcation (CivilComments) Truthfulness (Truthful QA) MMLU Benchmark (26 topics) Grounded conceptual mappings MMLU Benchmark (30 topics) Word in Context (WiC) benchmark Many BIG-Bench tasks (see Appendix E) Augmented prompting abilities (cid:114) (cid:114) (cid:114) (cid:114) (cid:114) (cid:114) (cid:114) (cid:114) (cid:114) (cid:114) (cid:114) (cid:114) (cid:114) Instruction following (ﬁnetuning) Scratchpad: 8-digit addition (ﬁnetuning) Using open-book knowledge for fact checking Chain-of-thought: Math word problems Chain-of-thought: StrategyQA Diﬀerentiable search index Self-consistency decoding Leveraging explanations in prompting Least-to-most prompting Zero-shot chain-of-thought reasoning Calibration via P(True) Multilingual chain-of-thought reasoning Ask me anything prompting 2.3E+22 3.1E+23 3.1E+23 1.3E+22 5.0E+23 5.0E+23 3.1E+23 5.0E+23 2.5E+24 Many 1.3E+23 8.9E+19 1.3E+22 1.3E+23 2.9E+23 3.3E+22 1.3E+23 5.0E+23 3.1E+23 3.1E+23 2.6E+23 2.9E+23 1.4E+22 175B 175B 7.1B 280B 280B 175B 540B Many 7.1B 280B 175B 175B GPT-3 Brown et al. (2020) GPT-3 Gopher Rae et al. (2021) Hendrycks et al. (2021a) GPT-3 Patel & Pavlick (2022) Chinchilla Hoﬀmann et al. (2022) PaLM Chowdhery et al. (2022) Many BIG-Bench (2022) FLAN Wei et al. (2022a) LaMDA Nye et al. (2021) Gopher Rae et al. (2021) LaMDA Wei et al. (2022b) PaLM Chowdhery et al. (2022) Tay et al. (2022b) LaMDA Wang et al. (2022b) Gopher GPT-3 GPT-3 Lampinen et al. (2022) Zhou et al. (2022) Kojima et al. (2022) Anthropic Kadavath et al. (2022) PaLM Shi et al. (2022) EleutherAI Arora et al. (2022) 5 Discussion We have seen that a range of abilities—in the few-shot prompting setup or otherwise—have thus far only been observed when evaluated on a suﬃciently large language model. Hence, their emergence cannot be predicted by simply extrapolating performance on smaller-scale models. Emergent few-shot prompted tasks are also unpredictable in the sense that these tasks are not explicitly included in pre-training, and we likely do not know the full scope of few-shot prompted tasks that language models can perform. This raises the question of whether further scaling could potentially endow even-larger language models with new emergent abilities. Tasks that language models cannot currently do are prime candidates for future emergence; for instance, there are dozens of tasks in BIG-Bench for which even the largest GPT-3 and PaLM models do not achieve above-random performance (see Appendix E.4). The ability for scale to unpredictably enable new techniques is not just theoretical. Consider the Word in Context (WiC) benchmark (Pilehvar & Camacho-Collados, 2019) shown in Figure 2H, as a historical example. Here, scaling GPT-3 to around 3 · 1023 training FLOPs (175B parameters) failed to unlock above-random one-shot prompting performance.3 Regarding this negative result, Brown et al. (2020) cited the model architecture of GPT-3 or the use of an autoregressive language modeling objective (rather than using a denoising training objective) as potential reasons, and suggested training a model of comparable size with bidirectional architecture as a remedy. However, later work found that further scaling a decoder-only language model was actually enough to enable above-random performance on this task. As is shown in Figure 2H, scaling PaLM (Chowdhery et al., 2022) from 3 · 1023 training FLOPs (62B parameters) to 3 · 1024 training 3GPT-3 does achieve slightly above-random performance on the dev set with few-shot instead of one-shot prompting (∼55%), but this above-random performance did not appear to be a result of scale and did not hold on the test set server. Published in Transactions on Machine Learning Research (08/2022) FLOPs (540B parameters) led to a signiﬁcant jump in performance, without the signiﬁcant architectural changes suggested by Brown et al. (2020). 5.1 Potential explanations of emergence Although there are dozens of examples of emergent abilities, there are currently few compelling explanations for why such abilities emerge in the way they do. For certain tasks, there may be natural intuitions for why emergence requires a model larger than a particular threshold scale. For instance, if a multi-step reasoning task requires l steps of sequential computation, this might require a model with a depth of at least O (l) layers. It is also reasonable to assume that more parameters and more training enable better memorization that could be helpful for tasks requiring world knowledge.4 As an example, good performance on closed-book question-answering may require a model with enough parameters to capture the compressed knowledge base itself (though language model-based compressors can have higher compression ratios than conventional compressors (Bellard, 2021)). It is also important to consider the evaluation metrics used to measure emergent abilities (BIG-Bench, 2022). For instance, using exact string match as the evaluation metric for long-sequence targets may disguise compounding incremental improvements as emergence. Similar logic may apply for multi-step or arithmetic reasoning problems, where models are only scored on whether they get the ﬁnal answer to a multi-step problem correct, without any credit given to partially correct solutions. However, the jump in ﬁnal answer accuracy does not explain why the quality of intermediate steps suddenly emerges to above random, and using evaluation metrics that do not give partial credit are at best an incomplete explanation, because emergent abilities are still observed on many classiﬁcation tasks (e.g., the tasks in Figure 2D–H). As an alternative evaluation, we measure cross-entropy loss, which is used in scaling laws for pre-training, for the six emergent BIG-Bench tasks, as detailed in Appendix A. This analysis follows the same experimental setup from BIG-Bench (2022) and aﬃrms their conclusions for the six emergent tasks we consider. Namely, cross-entropy loss improves even for small model scales where the downstream metrics (exact match, BLEU, and accuracy) are close to random and do not improve, which shows that improvements in the log-likelihood of the target sequence can be masked by such downstream metrics. However, this analysis does not explain why downstream metrics are emergent or enable us to predict the scale at which emergence occurs. Overall, more work is needed to tease apart what enables scale to unlock emergent abilities. 5.2 Beyond scaling Although we may observe an emergent ability to occur at a certain scale, it is possible that the ability could be later achieved at a smaller scale—in other words, model scale is not the singular factor for unlocking an emergent ability. As the science of training large language models progresses, certain abilities may be unlocked for smaller models with new architectures, higher-quality data, or improved training procedures. For example, there are 14 BIG-Bench tasks5 for which LaMDA 137B and GPT-3 175B models perform at near-random, but PaLM 62B in fact achieves above-random performance, despite having fewer model parameters and training FLOPs. While there is not an empirical study ablating every diﬀerence between PaLM 62B and prior models (the computational cost would be too high), potential reasons for the better performance of PaLM could include high-quality training data (e.g., more multilingual and code data than LaMDA) and architectural diﬀerences (e.g., split digit-encodings; see Section 2 in Chowdhery et al. (2022)). Another potentially way of unlocking emergence is through a diﬀerent pre-training objective—it was shown in Tay et al. (2022c) that a computationally-eﬃcient continued pre-training stage on a mixture-of-denoisers objective (Tay et al., 2022a) enabled emergent performance on several BIG-Bench tasks. Moreover, once an ability is discovered, further research may make the ability available for smaller scale models. Consider the nascent direction of enabling language models to follow natural language instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang et al., 2022, inter alia). Although Wei et al. (2022a) initially found that instruction-based ﬁnetuning only worked for 68B parameter or larger decoder-only 4Though note that encoding world knowledge in parameters is just one approach; there are others (e.g., Guu et al., 2020; Borgeaud et al., 2021). 5These tasks are enumerated in Appendix F. Published in Transactions on Machine Learning Research (08/2022) models, Sanh et al. (2022) induced similar behavior in a 11B model with an encoder-decoder architecture, which typically has higher performance after ﬁnetuning than decoder-only architectures (Wang et al., 2022a). As another example, Ouyang et al. (2022) proposed a ﬁnetuning and reinforcement learning from human feedback approach for the InstructGPT models, which enabled a 1.3B model to outperform much larger models in human-rater evaluations on a broad set of use cases. There has also been work on improving the general few-shot prompting abilities of language models (Gao et al., 2021; Schick & Schütze, 2021, inter alia). Theoretical and interpretability research (Wei et al., 2021a; Saunshi et al., 2021) on why a language modeling objective facilitates certain downstream behavior could in turn have implications on how to enable emergence beyond simply scaling. For instance, certain features of pre-training data (e.g., long-range coherence, having many rare classes) have also been shown to correlate with emergent few-shot prompting and could potentially enable it in smaller models (Xie et al., 2022; Chan et al., 2022), and few-shot learning can require certain model architectures in some scenarios (Chan et al., 2022). Computational linguistics work has further shown how threshold frequencies of training data can activate emergent syntactic rule-learning when model parameters and training FLOPs are held constant (Wei et al., 2021b), which has even been shown to have striking “aha” moments similar to those in the psycholinguistics literature (Abend et al., 2017; Zhang et al., 2021). As we continue to train language models, lowering the scale threshold for emergent abilities will become more important for making research on such abilities to available to the community more broadly (Bommasani et al., 2021; Ganguli et al., 2022; Liang et al., 2022). Naturally, there are limitations to a program consisting only of increasing scale (training compute, model parameters, and dataset size). For instance, scaling may eventually be bottle-necked by hardware constraints, and some abilities may not have emerged at this point. Other abilities may never emerge—for instance, tasks that are far out of the distribution of even a very large training dataset might not ever achieve any signiﬁcant performance. Finally, an ability could emerge and then plateau; in other words, there is no guarantee that scaling enables an ability to reach the desired level. 5.3 Another view of emergence While scale (e.g., training FLOPs or model parameters) has been highly correlated with language model performance on many downstream metrics so far, scale need not be the only lens to view emergent abilities. For example, the emergence of task-speciﬁc abilities can be analyzed as a function of the language model’s perplexity on a general text corpus such as WikiText103 (Merity et al., 2016). Figure 4 shows such a plot with WikiText103 perplexity of the language model on the x-axis and performance on the MMLU benchmark on the y-axis, side-by-side with plots of training FLOPs and model parameters on the x-axis. Because WikiText103 perplexity and training FLOPs happen to be highly correlated for the models considered here (Gopher and Chinchilla), the plots of emergent abilities look similar for both. However, this correlation between WikiText103 perplexity and scale may not hold in the future as new techniques beyond vanilla dense Transformer models are developed (e.g., retrieval-augmented models may have strong WikiText103 perplexity with less training compute and fewer model parameters (Borgeaud et al., 2021)). Also note that using WikiText103 perplexity to compare across model families can be complicated due to factors such as diﬀerences in training data composition. Overall, emergent abilities should probably be viewed as a function of many correlated variables. 5.4 Emergent risks Importantly, similar to how emergent abilities have been observed in the few-shot prompting setting without explicitly being included in pre-training, risks could also emerge (Bommasani et al., 2021; Steinhardt, 2021; Ganguli et al., 2022). For instance, societal risks of large language models such as truthfulness, bias, and toxicity are a growing area of research (Weidinger et al., 2021). Such risks are important considerations whether or not they can be precisely characterized as “emergent” based on the deﬁnition in §2, and, in some scenarios, do increase with model scale (see the Inverse Scaling Prize6). Since work on emergent abilities 6https://github.com/inverse-scaling/prize Published in Transactions on Machine Learning Research (08/2022) Training compute vs. model size 1024 1023 1022 1021 1B 10B 100B Model parameters WikiText103 ppl vs. training compute 1020 1022 1024 Training FLOPs WikiText103 ppl vs. model size 1B 10B 100B Model parameters MMLU MMLU 1020 1022 1024 Training FLOPs 1B 10B 100B Model parameters MMLU 20 15 10 7 5 WikiText103 ppl Chinchilla Gopher Random Figure 4: Top row: the relationships between training FLOPs, model parameters, and perplexity (ppl) on WikiText103 (Merity et al., 2016) for Chinchilla and Gopher. Bottom row: Overall performance on the massively multi-task language understanding benchmark (MMLU; Hendrycks et al., 2021a) as a function of training FLOPs, model parameters, and WikiText103 perplexity. incentivizes scaling language models, it is important to be aware of risks that increase with model scale even if they are not emergent. Here, we summarize several prior ﬁndings on the relationship between speciﬁc social risks and model scale. On WinoGender (Rudinger et al., 2017), which measures gender bias in occupations such as “nurse” or “electrician,” scaling has improved performance so far (Du et al., 2021; Chowdhery et al., 2022), though BIG-Bench (2022) found in BBQ bias benchmark (Parrish et al., 2022) that bias can increase with scaling for ambiguous contexts. As for toxicity, Askell et al. (2021) found that while larger language models could produce more toxic responses from the RealToxicityPrompts dataset (Gehman et al., 2020), this behavior could be mitigated by giving models prompts with examples of being “helpful, harmless, and honest.” For extracting training data from language models, larger models were found to be more likely to memorize training data (Carlini et al., 2021; 2022), though deduplication methods have been proposed and can simultaneously reduce memorization while improving performance (Kandpal et al., 2022; Lee et al., 2022a). The TruthfulQA benchmark (Lin et al., 2021) showed that GPT-3 models were more likely to mimic human falsehoods as they got larger, though Rae et al. (2021) later showed on a multiple-choice version that scaling Gopher to 280B enabled emergent performance substantially better than random. Beyond the above, emergent risks also include phenomena that might only exist in future language models or that have not yet been characterized in current language models. Some such behaviors, as discussed in detail in Hendrycks et al. (2021b), could be backdoor vulnerabilities, inadvertent deception, or harmful content synthesis. Approaches involving data ﬁltering, forecasting, governance, and automatically discovering harmful behaviors have been proposed for discovering and mitigating emergent risks (Bender et al., 2021; Weidinger et al., 2021; Steinhardt, 2021; Ganguli et al., 2022; Perez et al., 2022, inter alia). For a more detailed discussion of the risks of large language models, including emergent risks, see Bender et al. (2021); Steinhardt (2021); Bommasani et al. (2021); Ganguli et al. (2022). Published in Transactions on Machine Learning Research (08/2022) 5.5 Sociological changes Finally, the emergent abilities discussed here focus on model behavior and are just one of several types of emergence in NLP (Manning et al., 2020; Teehan et al., 2022). Another notable type of qualitative change is sociological, in which increasing scale has shifted how the community views and uses language models. For instance, NLP has historically focused on task-speciﬁc models (Jurafsky & Martin, 2009). Recently, scaling has led to an explosion in research on and development of models that are “general purpose” in that they are single models that aim to perform a range of tasks not explicitly encoded in the training data (e.g., GPT-3, Chinchilla, and PaLM) (Manning, 2022). One key set of results in the emergent sociological shift towards general-purpose models is when scaling enables a few-shot prompted general-purpose model to outperform prior state of the art held by ﬁnetuned task-speciﬁc models. As a few examples, GPT-3 175B achieved new state of the art on the TriviaQA and PiQA question-answering benchmarks (Brown et al., 2020); PaLM 540B achieved new state of the art on three arithmetic reasoning benchmarks (Chowdhery et al., 2022); and the multimodal Flamingo 80B model achieved new state of the art on six visual question answering benchmarks (Alayrac et al., 2022). In all of these cases, state-of-the-art performance was achieved by few-shot prompting a language model of unprecendented scale (scaling curves for these examples are shown in Appendix Figure 13). These abilities are not necessarily emergent since they have smooth, predictable scaling curves—however, they do underscore an emergent sociological shift towards general-purpose models in the NLP community. The ability for general-purpose models to perform unseen tasks given only a few examples has also led to many new applications of language models outside the NLP research community. For instance, language models have been used via prompting to translate natural language instructions into actions executable by robots (Ahn et al., 2022; Huang et al., 2022), interact with users (Coenen et al., 2021; Wu et al., 2021; 2022a; Lee et al., 2022b), and facilitate multi-modal reasoning (Zeng et al., 2022; Alayrac et al., 2022). Large language models have also been deployed in the real-world both in products, such as GitHub CoPilot,7 and directly as services themselves, such as OpenAI’s GPT-3 API.8 5.6 Directions for future work Future work on emergent abilities could involve train more-capable language models, as well as methods for better enabling language models to perform tasks. Some potential directions include but are not limited to the following. Further model scaling. Further scaling up models has so far appeared to increase the capabilities of language models, and is a straightforward direction for future work. However, simply scaling up language models is computationally expensive and requires solving substantial hardware challenges, and so other approaches will likely play a key role in the future of the emergent abilities of large language models. Improved model architectures and training. Improving model architecture and training procedures may facilitate high-quality models with emergent abilities while mitigating computational cost. One direction is using sparse mixture-of-experts architectures (Lepikhin et al., 2021; Fedus et al., 2021; Artetxe et al., 2021; Zoph et al., 2022), which scale up the number of parameters in a model while maintaining constant computational costs for an input. Other directions for better computational eﬃciency could involve variable amounts of compute for diﬀerent inputs (Graves, 2016; Dehghani et al., 2018), using more localized learning strategies than backpropagation through all weights in a neural network (Jaderberg et al., 2017), and augmenting models with external memory (Guu et al., 2020; Borgeaud et al., 2021; Wu et al., 2022b, inter alia). These nascent directions have already shown promise in many settings but have not yet seen widespread adoption, which will likely require further work. Data scaling. Training long enough on a large-enough dataset has been shown to be key for the ability of language models to acquire syntactic, semantic, and other world knowledge (Zhang et al., 2021; Wei et al., 2021b; Razeghi et al., 2022). Recently, Hoﬀmann et al. (2022) argued that prior work (Kaplan et al., 2020) 7https://copilot.github.com/ 8https://beta.openai.com/docs/introduction Published in Transactions on Machine Learning Research (08/2022) underestimated the amount of training data needed to train a compute-optimal model, underscoring the importance of training data. Collecting large datasets so that models can be trained for longer could allow a greater range of emergent abilities under a ﬁxed model size constraint. Better techniques for and understanding of prompting. Although few-shot prompting (Brown et al., 2020) is simple and eﬀective, general improvements to prompting may further expand the abilities of language models. For instance, simple modiﬁcations such as calibrating output probabilities (Zhao et al., 2021; Holtzman et al., 2021) or using a noisy channel (Min et al., 2022a) have improved performance on a range of tasks. Augmenting few-shot exemplars with intermediate steps (Reynolds & McDonell, 2021; Nye et al., 2021; Wei et al., 2022b) has also enabled models to perform multi-step reasoning tasks not possible in the standard prompting formulation from Brown et al. (2020). Moreover, better exploration of what makes prompting successful (Wei et al., 2021a; Xie et al., 2022; Min et al., 2022b; Olsson et al., 2022) could lead to insights on how to elicit emergent abilities at a smaller model scale. Suﬃcient understanding of why models work generally lags the development and popularization of techniques such as few-shot prompting, and it is also likely that the best practices for prompting will change as more-powerful models are developed over time. Frontier tasks. Although language models can perform a wide range of tasks, there are still many tasks that even the largest language models to date cannot perform with above-random accuracy. Dozens of such tasks from BIG-Bench are enumerated in Appendix E.4; these tasks often involve abstract reasoning (e.g., playing Chess, challenging math, etc). Future research could potentially investigate why these abilities have not yet emerged, and how to enable models to perform these tasks. Looking forward, another growing direction could be multilingual emergence; results on multilingual BIG-Bench tasks indicate that both model scale and training data play a role in emergence (e.g., Figure 2D shows that both using PaLM’s training dataset and scaling to 62B parameters is required for question-answering in Persian). Other frontier tasks could include prompting in multiple modalities (Alayrac et al., 2022; Ramesh et al., 2022). Understanding emergence. Beyond research on unlocking further emergence, an open question for future research is how and why emergent abilities occur in large language models. This paper conducted initial analyses regarding scaling of the cross-entropy loss on BIG-Bench (Appendix A.1), diﬀerent metrics for generative tasks (Appendix A.2), and which types of tasks emergence occurs (Appendix A.3 and Appendix B). These analyses did not provide complete answers to why emergence occurs or how to predict it. Future research could potentially analyze emergence in new ways (e.g., analyze the relationship between emergent tasks and similar data in training; create a synthetic task that requires multiple compositional sub-tasks and evaluate how each of those sub-tasks improve with scale and unlock emergence when combined). Overall, understanding emergence is an important direction because it could potentially allow us predict what abilities future models may have, as well as provide new insights into how to train more-capable language models. 6 Conclusions We have discussed emergent abilities of language models, for which meaningful performance has only been thus far observed at a certain computational scale. Emergent abilities can span a variety of language models, task types, and experimental scenarios. Such abilities are a recently discovered outcome of scaling up language models, and the questions of how they emerge and whether more scaling will enable further emergent abilities seem to be important future research directions for the ﬁeld of NLP. Broader Impact Statement In this paper, we surveyed results in the existing literature, without proposing new methods or models. As discussed in (§5), emergent abilities are unpredictable in several ways, and include emergent risks (§5.4). We believe these phenomena warrant careful study and raise important questions for the ﬁeld. Acknowledgments We thank Charles Sutton, Slav Petrov, Douglas Eck, Jason Freidenfelds, Jascha Sohl-Dickstein, Ethan Dyer, Dale Schuurmans, and Xavier Garcia for useful discussions and feedback on the manuscript. Published in Transactions on Machine Learning Research (08/2022) References Omri Abend, Tom Kwiatkowski, Nathaniel J Smith, Sharon Goldwater, and Mark Steedman. Bootstrapping language acquisition. Cognition, 164:116–143, 2017. URL https://homepages.inf.ed.ac.uk/ sgwater/papers/cognition17-bootstrapping.pdf. Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as I can, not as I say: Grounding language in robotic aﬀordances. arXiv preprint arXiv:2204.01691, 2022. URL https://arxiv.org/ abs/2204.01691. Jean-Baptiste Alayrac, Jeﬀ Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: A visual language model for few-shot learning. NeurIPS, 2022. URL https://arxiv.org/abs/2204.14198. Philip W. Anderson. More is diﬀerent: Broken symmetry and the nature of the hierarchical structure of science. Science, 177(4047):393–396, 1972. URL http://www.lanais.famaf.unc.edu.ar/cursos/ em/Anderson-MoreDifferent-1972.pdf. Simran Arora, Avanika Narayan, Mayee F Chen, Laurel J Orr, Neel Guha, Kush Bhatia, Ines Chami, Frederic Sala, and Christopher Ré. Ask me anything: A simple strategy for prompting language models. arXiv preprint arXiv:2210.02441, 2022. URL https://arxiv.org/abs/2210.02441. Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, et al. Eﬃcient large scale language modeling with mixtures of experts. arXiv preprint arXiv:2112.10684, 2021. URL https://arxiv.org/abs/2112.10684. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021. URL https://arxiv.org/abs/2112.00861. Fabrice Bellard. gpt2tc: Text completion and compression using GPT-2, 2021. URL https://bellard. org/libnc/gpt2tc.html. Accessed Apr. 26, 2022. Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 2021. URL https://dl.acm.org/doi/pdf/10.1145/3442188. 3445922. BIG-Bench. Beyond the imitation game: Measuring and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. URL https://arxiv.org/abs/2206.04615. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. URL https://arxiv.org/abs/2108. 07258. Sebastian Borgeaud, Arthur Mensch, Jordan Hoﬀmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. arXiv preprint arXiv:2112.04426, 2021. URL https: //arxiv.org/abs/2112.04426. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language mod- els are few-shot learners. NeurIPS, 2020. URL https://papers.nips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. Published in Transactions on Machine Learning Research (08/2022) Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language mod- els. USENIX Security, 2021. URL https://www.usenix.org/conference/usenixsecurity21/ presentation/carlini-extracting. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. arXiv preprint arXiv:2202.07646, 2022. URL https://arxiv.org/abs/2202.07646. Stephanie C.Y. Chan, Adam Santoro, Andrew K. Lampinen, Jane X. Wang, Aaditya Singh, Pierre H. Richemond, Jay McClelland, and Felix Hill. Data distributional properties drive emergent few-shot learning in transformers. arXiv preprint arXiv:2205.05055, 2022. URL https://arxiv.org/abs/2205.05055. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, et al. PaLM: Scaling language modeling with Pathways. arXiv preprint arXiv:2204.02311, 2022. URL https://arxiv.org/abs/2204.02311. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-ﬁnetuned language models. arXiv preprint arXiv:2210.11416, 2022. URL https://arxiv.org/abs/2210.11416. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training veriﬁers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. URL https://arxiv.org/abs/2110.14168. Andy Coenen, Luke Davis, Daphne Ippolito, Emily Reif, and Ann Yuan. Wordcraft: A human-AI collaborative editor for story writing. arXiv preprint arXiv:2107.07430, 2021. URL https://arxiv.org/abs/2107. 07430. Antonella Corradini and Timothy O’Connor. Emergence in science and philosophy, volume 6. Rout- ledge, 2010. URL https://books.google.com/books?hl=en&lr=&id=55RaBwAAQBAJ&oi= fnd&pg=PP1&dq=Emergence+in+science+and+philosophy&ots=2_8VNDXLfv&sig=1aisq_ WouF95Cx58WWMZ0Gq3RNk. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018. URL https://arxiv.org/abs/1807.03819. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirec- tional transformers for language understanding. NAACL, 2019. URL https://aclanthology.org/ N19-1423. Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. GLaM: Eﬃcient scaling of language models with mixture-of-experts. ICML, 2021. URL https://arxiv.org/abs/2112.06905. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and eﬃcient sparsity. arXiv preprint arXiv:2101.03961, 2021. URL https://arxiv.org/ abs/2101.03961. Stephanie Forrest. Emergent computation: Self-organizing, collective, and cooperative phenomena in natural and artiﬁcial computing networks. Physica D: Nonlinear Phenomena, 42(1-3):1–11, 1990. URL https://www.sciencedirect.com/science/article/abs/pii/016727899090063U. Deep Ganguli, Danny Hernandez, Liane Lovitt, Nova DasSarma, Tom Henighan, Andy Jones, Nicholas Joseph, Jackson Kernion, Ben Mann, Amanda Askell, et al. Predictability and surprise in large generative models. arXiv preprint arXiv:2202.07785, 2022. URL https://arxiv.org/abs/2202.07785. Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. ACL, 2021. doi: 10.18653/v1/2021.acl-long.295. URL https://aclanthology.org/2021.acl-long.295. Published in Transactions on Machine Learning Research (08/2022) Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of EMNLP, 2020. doi: 10.18653/v1/ 2020.ﬁndings-emnlp.301. URL https://aclanthology.org/2020.findings-emnlp.301. Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016. URL https://arxiv.org/abs/1603.08983. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-augmented language model pre-training. ICML, 2020. URL https://arxiv.org/abs/2002.08909. David A. Harper and Paul A. Lewis. New perspectives on emergence in economics. New Per- URL https://www.sciencedirect. spectives on Emergence in Economics, pp. 2–3, 2012. com/science/article/pii/S0167268112000200?casa_token=fLs2nCYo_64AAAAA: H2sSpSygJmEqXgmpM4jLyeppph3C4TgEsaSXm5RkOpT0r4q2A1x9Su3u4uycK4sIC6a8NdLiSw. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. ICLR, 2021a. URL https://openreview.net/ forum?id=d7KBjmI3GmQ. Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved problems in ML safety. arXiv preprint arXiv:2109.13916, 2021b. URL https://arxiv.org/abs/2109.13916. Jordan Hoﬀmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. NeurIPS, 2022. URL https://arxiv.org/abs/2203.15556. Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. Surface form competition: Why the highest probability answer isn’t always right. EMNLP, 2021. URL https://aclanthology. org/2021.emnlp-main.564. Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207, 2022. URL https://arxiv.org/pdf/2201.07207. Bernardo A. Huberman and Tad Hogg. Phase transitions in artiﬁcial intelligence systems. Artiﬁcial Intelligence, 33(2):155–171, 1987. URL https://www.sciencedirect.com/science/article/ abs/pii/0004370287900336. Harold Y. Hwang, Yoh Iwasa, Masashi Kawasaki, Bernhard Keimer, Naoto Nagaosa, and Yoshinori Tokura. Emergent phenomena at oxide interfaces. Nature Materials, 11(2):103–113, 2012. URL https://www. nature.com/articles/nmat3223. Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, David Silver, and Koray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. ICML, 2017. URL https://arxiv.org/abs/1608.05343. Dan Jurafsky and James H. Martin. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Prentice Hall series in Artiﬁcial Intelligence. Pearson Prentice Hall, 2009. ISBN 9780131873216. URL https://books.google.com/books?id= fZmj5UNK8AQC. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatﬁeld Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022. URL https://arxiv.org/abs/2207.05221. Nikhil Kandpal, Eric Wallace, and Colin Raﬀel. Deduplicating training data mitigates privacy risks in language models. ICML, 2022. URL https://arxiv.org/abs/2202.06539. Published in Transactions on Machine Learning Research (08/2022) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeﬀrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. URL https://arxiv.org/abs/2001.08361. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. NeurIPS, 2022. URL https://arxiv.org/abs/2205.11916. Andrew K. Lampinen, Ishita Dasgupta, Stephanie C.Y. Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L. McClelland, Jane X. Wang, and Felix Hill. Can language models learn from explanations in context? Findings of EMNLP, 2022. URL https://arxiv.org/abs/2204.02329. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. ACL, 2022a. URL https://arxiv.org/abs/2107.06499. Mina Lee, Percy Liang, and Qian Yang. Coauthor: Designing a human-AI collaborative writing dataset for exploring language model capabilities. CHI, 2022b. URL https://arxiv.org/abs/2201.06796. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. ICLR, 2021. URL https://openreview.net/forum?id=qrwe7XHTmYb. Percy Liang. Semi-supervised learning for natural language. PhD thesis, Massachusetts Institute of Technology, 2005. URL https://www-cs.stanford.edu/~pliang/papers/meng-thesis.pdf. Percy Liang, Rishi Bommasani, Kathleen A. Creel, and Rob Reich. The time is now to develop community norms for the release of foundation models, 2022. URL https://crfm.stanford.edu/2022/05/17/ community-norms.html. Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. URL https://arxiv.org/abs/2109.07958. Christopher D. Manning. Human language understanding & reasoning. Daedalus, 151(2):127–138, 2022. URL https://www.amacad.org/publication/human-language-understanding-reasoning. Christopher D. Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, and Omer Levy. Emergent linguistic structure in artiﬁcial neural networks trained by self-supervision. Proceedings of the National Academy of Sciences, 117(48):30046–30054, 2020. URL https://www.pnas.org/doi/10.1073/pnas. 1907367117. Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018. URL https://arxiv. org/abs/1806.08730. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. URL https://huggingface.co/datasets/wikitext. Scott Miller, Jethran Guinness, and Alex Zamanian. Name tagging with word clusters and discriminative training. In NAACL, 2004. URL https://aclanthology.org/N04-1043. Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Noisy channel language model prompting for few-shot text classiﬁcation. ACL, 2022a. URL https://arxiv.org/abs/2108.04106. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle- moyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022b. URL https://arxiv.org/abs/2202.12837. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021. URL https://openreview.net/forum?id=iedYJm92o0a. Published in Transactions on Machine Learning Research (08/2022) Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Transformer Circuits, 2022. URL https: et al. //transformer-circuits.pub/2022/in-context-learning-and-induction-heads/ index.html. In-context learning and induction heads. Long Ouyang, Jeﬀ Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022. URL https://arxiv.org/abs/2203.02155. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evaluation of machine translation. In ACL, 2002. URL https://aclanthology.org/P02-1040.pdf. Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel Bowman. BBQ: A hand-built bias benchmark for question answering. In Findings of ACL, 2022. URL https://arxiv.org/abs/2110.08193. Roma Patel and Ellie Pavlick. Mapping language models to grounded conceptual spaces. ICLR, 2022. URL https://openreview.net/forum?id=gJcEM8sxHK. Ethan Perez, Saﬀron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoﬀrey Irving. Red teaming language models with language models. arXiv preprint arXiv:2202.03286, 2022. URL https://arxiv.org/abs/2202.03286. Mohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for evaluat- ing context-sensitive meaning representations. NAACL, 2019. URL https://aclanthology.org/ N19-1128. Alec Radford, Jeﬀrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Language models are unsupervised multitask learners. https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_ are_unsupervised_multitask_learners.pdf. OpenAI blog, 2019. 1(8), et al. Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoﬀmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training Gopher. arXiv preprint arXiv:2112.11446, 2021. URL https://arxiv.org/abs/ 2112.11446. Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 2020. URL https://jmlr.org/papers/v21/20-074.html. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. URL https://arxiv.org/ abs/2204.06125. Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206, 2022. URL https://arxiv.org/ abs/2202.07206. Laria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the few-shot paradigm. Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, 2021. URL https://arxiv.org/abs/2102.07350. Rachel Rudinger, Chandler May, and Benjamin Van Durme. Social bias in elicited natural language inferences. In Proceedings of the First ACL Workshop on Ethics in Natural Language Processing, 2017. URL https://aclanthology.org/W17-1609. Victor Sanh, Albert Webson, Colin Raﬀel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaﬃn, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. ICLR, 2022. URL https://openreview.net/forum?id=9Vrb9D0WI4. Published in Transactions on Machine Learning Research (08/2022) Nikunj Saunshi, Sadhika Malladi, and Sanjeev Arora. A mathematical exploration of why language models help solve downstream tasks. ICLR, 2021. URL https://arxiv.org/abs/2010.03648. Timo Schick and Hinrich Schütze. It’s not just size that matters: Small language models are also few-shot learners. NAACL, June 2021. URL https://aclanthology.org/2021.naacl-main.185. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models are multilingual chain-of-thought reasoners. arXiv preprint arXiv:2210.03057, 2022. URL https://arxiv.org/abs/ 2210.03057. Jacob Steinhardt. risks models, on-the-risks-of-emergent-behavior-in-foundation-models/. Accessed Apr 13, 2022. October emergent foundation https://bounded-regret.ghost.io/ behavior 2021. Jacob Steinhardt. Future ml systems will be qualitatively diﬀerent, 2022. URL https://bounded-regret. Accessed May 20, ghost.io/future-ml-systems-will-be-qualitatively-different/. 2022. Mirac Suzgun, Nathan Scales, Nathaneal Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny ZHou, and Jason Wei. Challenging BIG-Bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. URL https: //arxiv.org/abs/2210.09261. Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, and Donald Metzler. Unifying language learning paradigms. arXiv preprint arXiv:2205.05131, 2022a. URL https://arxiv.org/abs/2205.05131. Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, et al. Transformer memory as a diﬀerentiable search index. arXiv preprint arXiv:2202.06991, 2022b. URL https://arxiv.org/abs/2202.06991. Yi Tay, Jason Wei, Hyung Won Chung, Vinh Q Tran, David R So, Siamak Shakeri, Xavier Garcia, Huaixiu Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, et al. Transcending scaling laws with 0.1% extra compute. arXiv preprint arXiv:2210.11399, 2022c. URL https://arxiv.org/abs/2210.11399. Ryan Teehan, Miruna Clinciu, Oleg Serikov, Eliza Szczechla, Natasha Seelam, Shachar Mirkin, and Aaron Gokaslan. Emergent structures and training dynamics in large language models. In ACL Big Science Workshop, 2022. URL https://aclanthology.org/2022.bigscience-1.11/. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. LaMDA: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022. URL https://arxiv.org/abs/2201.08239. Trieu H. Trinh and Quoc V. Le. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847, 2018. URL https://arxiv.org/abs/1806.02847. Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay, and Colin Raﬀel. What language model architecture and pretraining objective work best for zero-shot generalization? ICML, 2022a. URL https://arxiv.org/abs/2204.05832. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022b. URL https: //arxiv.org/abs/2203.11171. Colin Wei, Sang Michael Xie, and Tengyu Ma. Why do pretrained language models help in downstream tasks? An analysis of head and prompt tuning. NeurIPS, 2021a. URL https://openreview.net/ forum?id=MDMV2SxCboX. Published in Transactions on Machine Learning Research (08/2022) Jason Wei, Dan Garrette, Tal Linzen, and Ellie Pavlick. Frequency eﬀects on syntactic rule learning in transformers. EMNLP, 2021b. doi: 10.18653/v1/2021.emnlp-main.72. URL https://aclanthology. org/2021.emnlp-main.72. Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. ICLR, 2022a. URL https: //openreview.net/forum?id=gEZrGCozdqR. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. NeurIPS, 2022b. URL https://arxiv. org/abs/2201.11903. Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griﬃn, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359, 2021. URL https://arxiv.org/abs/2112.04359. Tongshuang Wu, Michael Terry, and Carrie J. Cai. AI chains: Transparent and controllable human-AI interaction by chaining large language model prompts. arXiv preprint arXiv:2110.01691, 2021. URL https://arxiv.org/abs/2110.01691. Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeﬀ Gray, Alejandra Molina, Michael Terry, and Carrie J. Cai. PromptChainer: Chaining large language model prompts through visual programming. arXiv preprint arXiv:2203.06566, 2022a. URL https://arxiv.org/abs/2203.06566. Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022b. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. ICLR, 2022. URL https://arxiv.org/abs/2111.02080. Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022. URL https://arxiv. org/abs/2204.00598. Yian Zhang, Alex Warstadt, Xiaocheng Li, and Samuel R. Bowman. When do you need billions of words of pretraining data? In ACL, 2021. doi: 10.18653/v1/2021.acl-long.90. URL https://aclanthology. org/2021.acl-long.90. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. ICML, 2021. URL https://arxiv.org/abs/2102.09690. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022. URL https://arxiv.org/abs/2205.10625. Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeﬀ Dean, Noam Shazeer, and William Fedus. Designing eﬀective sparse expert models. arXiv preprint arXiv:2202.08906, 2022. URL https: //arxiv.org/abs/2202.08906. Published in Transactions on Machine Learning Research (08/2022) A BIG-Bench analysis A.1 Cross-entropy loss analysis Here we study how scaling curves may appear diﬀerently depending on the evaluation metric used to measure performance. We will focus on the six few-shot prompted BIG-Bench tasks that we consider emergent for LaMDA models. Three of these tasks are generative and use Exact Match (EM) or BLEU (Papineni et al., 2002) as the evaluation metric. The other three tasks are classiﬁcation and use accuracy (acc) as the evaluation metric. In the scaling curves for these tasks, peformance in EM/BLEU/acc is close to random for small models (≤1022 FLOPs / ≤27B params). We will compare these scaling curves against alternative plots that have a diﬀerent y-axis measured by cross-entropy loss. Cross-entropy loss diﬀers from EM/BLEU/acc in that it captures improvements in performance (the predicted distribution getting closer to ground truth) even when the EM/BLEU/acc is random. For example, if two examples are both wrong as measured by EM/BLEU/acc, one example may be closer to the ground truth in terms of probabilities, and this information is captured by the cross-entropy loss. These plots are expected to look like one of the following: • Outcome 1: For the model scales where EM/BLEU/acc is random, cross-entropy loss also does not improve as scale increases. This outcome implies that for these scales, the model truly does not get any better at the tasks. • Outcome 2: For the model scales where EM/BLEU/acc is random, cross-entropy loss does improve. This outcome implies that the models do get better at the task, but these improvements are not reﬂected in the downstream metric of interest. The broader implication is that scaling small models improves the models in a way that is not reﬂected in EM/BLEU/Acc, and that there is some critical model scale where these improvements enable the downstream metric to increase to above random as an emergent ability. We ﬁnd that all six BIG-Bench tasks fall under Outcome 2, and detail this analysis below. Overall, the conclusion from this analysis is that small models do improve in some ways that downstream metrics that EM/BLEU/Acc do not capture. However, these tasks are still considered emergent, and this analysis does not provide any straightforward indicators of how to predict such emergent behaviors. A.1.1 Generative tasks Figure 5 shows the cross-entropy loss on the three generative BIG-Bench tasks (modiﬁed arithmetic, IPA transliterate, and word unscramble) alongside the downstream evaluation metrics used in Figure 2. For all three tasks, notice that while the error rate is nearly 100% for small models (≤1022 FLOPs / ≤27B params), the cross-entropy loss does actually improve for these model sizes. At the point of emergence as measured by error rate, we also see an “elbow” in performance improvement for cross-entropy loss. A.1.2 Classiﬁcation tasks Figure 6 (middle row) shows the cross-entropy loss of the three classiﬁcation BIG-Bench tasks. Similar to the generative tasks, when the error rate is close to random, cross-entropy loss consistently still improves for models trained with more compute. This again shows that performance as computed by accuracy can mask consistent improvements in the likelihood of the target sequences. We also perform an additional analysis of the multiple choice emergent tasks in Figure 6 (bottom row), which shows the log probabilities of the correct response and incorrect response(s). We ﬁnd that the cross- entropy loss decreases for both the correct and incorrect responses in the three emergent multiple choice tasks. Counterintuitively, both log-probabilities can decrease in tandem even when the probability across all available multiple choice responses is normalized. The reason is that larger models produce less-extreme probabilities (i.e., values approaching 0 or 1) and therefore the average log-probabilities have fewer extremely Published in Transactions on Machine Learning Research (08/2022) T = 0 T = 1 Random Modiﬁed arithmetic IPA transliterate Word unscramble 1020 1022 1024 1020 1022 1024 1020 1022 1024 1020 1022 1024 1020 1024 1022 Training FLOPs 1020 1022 1024 Figure 5: Adjacent plots for error rate and cross-entropy loss on three emergent generative tasks in BIG-Bench for LaMDA. We show error rate for both greedy decoding (T = 0) as well as random sampling (T = 1). Error rate is (1 - exact match score) for modiﬁed arithmetic and word unscramble, and (1 - BLEU score) for IPA transliterate. small values. However, we note that for each of these three tasks, that the average log-probability of the correct and incorrect responses eventually deviates at a certain scale, during which performance on the task increases substantially. Published in Transactions on Machine Learning Research (08/2022) Logical arguments Sports understanding Figure of speech LaMDA Random 1020 1022 1024 1020 1022 1024 1020 1022 1024 0.05 0.05 1020 1022 1024 1020 1022 1024 1020 1022 1024 Correct response Incorrect response(s) 0.05 0.05 1020 1022 1024 1020 1022 Model scale (training FLOPs) 1024 1020 1022 1024 0.05 0.05 Figure 6: Adjacent plots for error rate, cross-entropy loss, and log probabilities of correct and incorrect responses on three classiﬁcation tasks on BIG-Bench that we consider to demonstrate emergent abilities. Logical arguments only has 32 samples, which may contribute to noise. Error rate is (1 - accuracy). Published in Transactions on Machine Learning Research (08/2022) A.2 Diﬀerent metrics for generative tasks In §5.1 we asked whether the apparent emergent abilities on generative tasks were due to using a particular metric such as exact string match, which does not award partially correct sequences. Here, we show three emergent generative BIG-Bench tasks using all evaluation metrics provided by BIG-Bench, which includes metrics such as BLEU, ROUGE, and BLEURT, that award partial credit for answers that do not exactly match the target. For all three tasks, the emergent behavior appears to be independent of which evaluation metric is used. Hence, we conclude that using exact string match instead of another evaluation metric that awards partial credit is not a complete explanation of emergence on generative tasks. Two emergent generative BIG-Bench tasks, word unscramble and repeat copy logic, are excluded here because exact match is the only most sensible evaluation metric for those tasks, which measure the ability to manipulate words in the input (and hence metrics like BLEU and ROUGE that give word-level partial credit are not valid). (A) Mod. arithmetic (B) IPA transliterate (C) Periodic elements Exact match ROUGE1 ROUGE2 ROUGE L-Sum BLEURT BLEU Sequence F1 Multiple choice grade 1018 1020 1022 1024 1018 1020 1022 1024 1018 1020 1022 1024 Figure 7: Multiple evaluation metrics for emergent BIG-Bench tasks that are generative in nature. For all three tasks, emergent behavior is apparent for all evaluation metrics. A.3 BIG-Bench task analysis BIG-Bench contains over 200 tasks, and each task has associated keywords identiﬁed by the authors who submitted the task (e.g., “common sense”, “multilingual”). Given this, we asked the question, which types of BIG-Bench tasks are more likely to be emergent (compared with scaling smoothly)? For this analysis, we manually classiﬁed all 210 BIG-Bench tasks as thus far emergent or not. We used the deﬁnition of emergence given in §3, which is that the task had near-random performance until a certain scale, after which performance increases to substantially above random (as opposed to smoothly increasing). Because this deﬁnition is potentially subjective based on the deﬁnition of “near-random” (and any heuristic we decide on would encode these subjective biases), two co-authors of the paper worked together and agreed with conﬁdence on all the tasks labeled as emergent. For full transparency, this set of annotations is listed in Appendix E. In Figure 8, we show the number of tasks that are emergent for each keyword in BIG-Bench. Furthermore, we stratify them by tasks that ﬁrst emerged with LaMDA 137B or GPT-3 175B, as well as tasks that were not emergent until using PaLM models. The non-emergent tasks in this plot include either “smoothly increasing” tasks (performance predictably increased with model size) or “ﬂat” tasks (all models achieved approximately random performance). The remaining 40 BIG-Bench tasks not included in this chart did not ﬁt into any of the above categories (e.g., too noisy due to very few eval examples, performance not correlated with model scale, etc.). Since the number of tasks per keyword varied substantially among keywords, and most keywords had less than twenty tasks, the “most emergent” keywords diﬀered depending on whether we compare number of emergent tasks or percentage of emergent tasks per keyword. Tracking the absolute number of emergent tasks per keyword is problematic since it eﬀectively just captures the most common keywords used across BigBench. We therefore tracked which keywords had the highest percent of emergent tasks, which were analogical reasoning, word sense disambiguation, truthfulness, social reasoning, and emotional understanding. While one might expect a priori that reasoning-related tasks would more likely to be emergent, only two of the top ﬁve tasks were reasoning and other keyword tags like logical reasoning and causal reasoning did not Published in Transactions on Machine Learning Research (08/2022) have a particularly high fraction of emergent tasks. Moreover, arithmetic and mathematics had relatively low percentage of emergent tasks, which was unexpected since some of the earliest examples of emergence were on arithmetic (Brown et al., 2020). Overall, there are no clear trends for which types of tasks are most emergent. Finally, examining which keywords have the most tasks with ﬂat scaling curves can also align with prior intuitions. For instance, visual reasoning has the largest fraction of tasks with ﬂat scaling curves (8/13), since language models are not designed for visual reasoning. Other categories with a large fraction of ﬂat scaling curve tasks are non-language, repeated interaction, context length, computer code, and multi-step—all targeting weaknesses of large language models. These ﬂat categories could be directions for future work in emergence in large language models. Emergent with LaMDA/GPT Emergent with PaLM Smoothly increasing Flat (no model better than random) co m m on-sense creativity context-length m ulti-step m ultilingual do m ain-speciﬁc non-language analogical-reasoning parap hrase non-E nglish w ord-sense-disa m biguation causal-reasoning reading-co m prehension logical-reasoning theory-of- m in d visual-reasoning e m otional-u n derstan ding nu m erical-response context-free-question-ans w ering contextual-question-ans w ering repeated-interaction im plicit-reasoning narrative-u n derstan ding lo w-resource-language social-reasoning co m p uter-code m arization out-of-distrib ution translation hu m an-like-behavior truthfulness m athe m atics arith m etic su m Keyword Tag Figure 8: Proportion of emergent tasks for keywords in BIG-Bench (each task can be associated with multiple keywords). We only included keywords with at least ﬁve tasks. Smoothly increasing: performance improved predictably as model scale increased. Emergent with LaMDA/GPT: performance was near-random until used with LaMDA 137B or GPT-3 175B. Emergent with PaLM: performance was near-random for all previous models, until using a PaLM model (8B, 62B, or 540B). Flat: no model performs better than random. Published in Transactions on Machine Learning Research (08/2022) B Further MMLU analysis In §5.3, we saw how emergent performance on MMLU for Gopher and Chinchilla could be viewed as a function of training FLOPs, model parameters, and WikiText103 perplexity. Because MMLU is actually a suite of 57 topics spanning four categories, we ask the question of whether certain categories were more conducive to emergence than others. This is similar in nature to the BIG-Bench analysis done in the prior section (Appendix A.3). One diﬀerence here is that the MMLU categories are mutually exclusive—each topic only has one category, whereas a single BIG-Bench task often had multiple keyword tags. However, there are only four categories and 57 tasks for MMLU (compared with 200+ tasks and dozens of keywords for BIG-Bench). In Figure 10, we stratify the performance of MMLU among the four categories given in the benchmark (Humanities, STEM, Social Science, and other), and plot them with multiple x-axes: training FLOPs, model parameters, and WikiText103 perplexity. It is clear that Social Science and Humanities have the largest jump in performance from the second-largest to the largest model, and STEM has the smallest jump in performance. For a given x-axis (training FLOPs, model parameters, WikiText103 ppl), all four categories had similar plot shapes. This result is also summarized in Figure 9. Performance of second-largest model Chinchilla: Humanities Chinchilla: STEM Chinchilla: Other Chinchilla: Social Science Gopher: Humanities Gopher: STEM Gopher: Other Gopher: Social Science Random performance (25%) Figure 9: Performance of largest Chinchilla and Gopher models (70B and 280B, respectively) compared with the second-largest model (7B parameters for both Chiinchlla and Gopher). The 7B Chinchilla and Gopher models perform around random (25%) for all four categories. So the categories that improved the most from 7B to 70B/280B are humanities and social science, whereas STEM (Science, Technology, Engineering, and Mathematics) improved the least. Published in Transactions on Machine Learning Research (08/2022) Chinchilla Gopher Random MMLU: Humanities 1B 10B 100B Model parameters 20 15 10 7 5 WikiText103 ppl 1020 1022 1024 Training FLOPs MMLU: Science, Technology, Engineering, and Math (STEM) 1020 1022 1024 Training FLOPs 1B 10B 100B Model parameters 20 15 10 7 5 WikiText103 ppl MMLU: Other 1020 1022 1024 Training FLOPs 1B 10B 100B Model parameters 20 15 10 7 5 WikiText103 ppl MMLU: Social Science 1020 1022 1024 Training FLOPs 1B 10B 100B Model parameters 20 15 10 7 5 WikiText103 ppl Figure 10: Emergence of Chinchilla and Gopher on MMLU. In the four rows, performance is stratiﬁed into four supercategories. For both Chinchilla and Gopher, Social Science had the highest level of emergence while STEM was the least emergent. Published in Transactions on Machine Learning Research (08/2022) C All Model Details Table 2 below summarizes the parameter count, number of training tokens, and the training FLOPs for the models highlighted in our work. The models span from the smallest LaMDA model with 2.1M parameters to the largest PaLM model with 540B parameters and 2.5E+24 training FLOPs—roughly 8x the computational budget of GPT-3. Table 2: Parameters, training examples, and training FLOPs of large language models. Model GPT-3 LaMDA Gopher Chinchilla PaLM Anthropic LM Parameters Train tokens Train FLOPs 125M 350M 760M 1.3B 2.7B 6.7B 175B 2.1M 134M 262M 453M 1.1B 2.1B 3.6B 8.6B 137B 417M 1.4B 7.1B 280B 417M 1.4B 7.1B 540B 800M 300B 300B 300B 300B 300B 300B 300B 300B 262B 313B 262B 170B 264B 150B 142B 137B 136B 132B 132B 292B 674B 300B 300B 300B 325B 314B 314B [sic] 199B 1.34T 780B 780B 780B 850B 850B 850B 850B 2.25E+20 6.41E+20 1.37E+21 2.38E+21 4.77E+21 1.20E+22 2.31E+22 3.14E+23 3.30E+18 3.16E+19 8.90E+19 1.37E+20 4.16E+20 4.08E+20 9.11E+20 1.72E+21 2.96E+21 6.78E+21 2.30E+22 1.20E+23 5.54E+23 7.51E+20 2.52E+21 1.28E+22 5.46E+23 7.86E+20 2.63E+21 8.47E+21 5.63E+23 3.74E+22 2.90E+23 2.53E+24 4.08E+21 1.53E+22 6.12E+22 2.65E+22 Published in Transactions on Machine Learning Research (08/2022) D Scaling with Parameter Count Figures 11, 12, and 13 shows emergent abilities with an x-axis of number of model parameters. LaMDA GPT-3 Gopher Chinchilla PaLM Random (A) Mod. arithmetic (B) IPA transliterate (C) Word unscramble (D) Persian QA 100B 100B 100B 10M 1B 100B (E) TruthfulQA (F) Grounded mappings (G) Multi-task NLU (H) Word in context 100M 10B 100M 10B 100M 10B 100M 10B Model scale (number of parameters) Figure 11: Eight examples of emergence in the few-shot prompting setting. Each point is a separate model. The ability to perform a task via few-shot prompting is emergent when a language model achieves random performance until a certain scale, after which performance signiﬁcantly increases to well-above random. Note that models with more parameters also typically use more training compute—hence, we show an analogous ﬁgure with training FLOPs instead of number of model parameters as the x-axis in Figure 2. A–D: BIG-Bench (2022), 2-shot. E: Lin et al. (2021) and Rae et al. (2021). F: Patel & Pavlick (2022). G: Hendrycks et al. (2021a), Rae et al. (2021), and Hoﬀmann et al. (2022). H: Brown et al. (2020), Hoﬀmann et al. (2022), and Chowdhery et al. (2022) on the WiC benchmark (Pilehvar & Camacho-Collados, 2019). Published in Transactions on Machine Learning Research (08/2022) (A) Math word problems (B) Instruction following (C) 8-digit addition Chain of thought No chain of thought Instruction tuning instruction tuning Scratchpad scratchpad 1B 10B 100B 1B 10B 100B 10M 100M 1B Model scale (number of parameters) ) (D) Calibration Letter choices 10B 100B Figure 12: Specialized prompting or ﬁnetuning methods can be emergent in that they do not have a positive eﬀect until a certain model scale. A: Wei et al. (2022b). B: Wei et al. (2022a). C: Nye et al. (2021). D: Kadavath et al. (2022). The model shown in A-C is LaMDA (Thoppilan et al., 2022), and the model shown in D is from Anthropic. Prior SOTA (pretrain–ﬁnetune) Few-shot prompting (A) TriviaQA (GPT-3) (B) Physical QA (GPT-3) (C) GSM8K (PaLM) (D) OKVQA (Flamingo) 100B 100B 8B 62B 540B 3B 9B Model scale (number of parameters) Figure 13: On some benchmarks, task-general models (not explicitly trained to perform a task) surpass prior state-of-the-art performance held by a task-speciﬁc model. A & B: Brown et al. (2020). C: Chowdhery et al. (2022). D: Alayrac et al. (2022). Published in Transactions on Machine Learning Research (08/2022) E BIG-Bench Task Classiﬁcation This appendix contains the task classiﬁcation annotations used for Figure 8 in Appendix A.3. Each task only appears in a single category. That is, if a task was initially emergent with GPT-3 or LaMDA, we excluded it from the PaLM emergence category. Notably, Appendix E.4 lists the tasks where no model performs better than random (i.e., ﬂat scaling curve). These tasks are potential candidates for future emergence, since a model in the future might achieve above-random performance on them. E.1 Smoothly increasing abstract narrative understanding, auto categorization, bbq lite json, cause and eﬀect, chess state tracking, con- lang translation, context deﬁnition alignment, contextual parametric knowledge conﬂicts, coqa conversational question answering, cryobiology spanish, date understanding, emojis emotion prediction, empirical judgments, entailed polarity, evaluating information essentiality, forecasting subquestions, gem, general knowledge, hindi question answering, human organs senses, implicatures, implicit relations, intent recognition, linguistic mappings, list functions, matrixshapes, mult data wrangling, multiemo, natural instructions, nonsense words grammar, object counting, operators, penguins in a table, physics, polish sequence labeling, qa wikidata, reasoning about colored objects, rephrase, riddle sense, sentence ambiguity, similarities abstraction, simp turing concept, simple arithmetic, simple arithmetic json, simple arithmetic json multiple choice, simple arithmetic json subtasks, simple arithmetic multiple targets json, simple ethical questions, squad shifts, subject verb agreement, swedish to german proverbs, undo permutation, unit conversion, unnatural in context learning, bridging anaphora resolution barqa, disﬂ qa, novel concepts, periodic elements E.2 Emergent with GPT-3 or LaMDA analytic entailment, codenames, common morpheme, fact checker, ﬁgure of speech detection, gender inclusive sentences german, hindu knowledge, international phonetic alphabet transliterate, irony identiﬁcation, logical args, logical deduction, misconceptions, modiﬁed arithmetic, phrase relatedness, physical intuition, question answer creation, repeat copy logic, self evaluation tutoring, social iqa, sports understanding, strange stories, strategyqa, swahili english proverbs, word sorting, word unscrambling E.3 Emergent wih PaLM anachronisms, analogical similarity, ascii word recognition, auto debugging, causal judgment, code line description, conceptual combinations, crass ai, cryptonite, cs algorithms, disambiguation qa, elementary math qa, emoji movie, english proverbs, english russian proverbs, geometric shapes, goal step wikihow, gre reading comprehension, hinglish toxicity, hyperbaton, identify odd metaphor, international phonetic alphabet nli, language identiﬁcation, linguistics puzzles, logic grid puzzle, logical fallacy detection, logical sequence, metaphor boolean, metaphor understanding, movie dialog same or diﬀerent, odd one out, parsinlu qa, parsinlu reading comprehension, physics questions, question selection, snarks, suﬃcient information, temporal sequences, timedial, understanding fables, unit interpretation, vitaminc fact veriﬁcation E.4 Flat (no model better than random) abstraction and reasoning corpus, authorship veriﬁcation, checkmate in one, chinese remainder theorem, cifar10 classiﬁcation, color, com2sense, cycled letters, discourse marker prediction, formal fallacies syllogisms negation, hhh alignment, kanji ascii, kannada, key value maps, language games, mathematical induction, minute mysteries qa, misconceptions russian, mnist ascii, multistep arithmetic, navigate, paragraph segmentation, play dialog same or diﬀerent, presuppositions as nli, program synthesis, python programming challenge, real or fake text, roots optimization and games, salient translation error detection, self awareness, semantic parsing in context sparc, semantic parsing spider, simple text editing, sudoku, symbol interpretation, talkdown, tense, text navigation game, topical chat, tracking shuﬄed objects, twenty questions, web of lies, which wiki edit, winowhy, word problems on sets and graphs Published in Transactions on Machine Learning Research (08/2022) E.5 Other Better than random and not correlated with scale: boolean expressions, crash blossom, dynamic counting, entailed polarity hindi, epistemic reasoning, factuality of summary, fantasy reasoning, gender sensitivity chinese, gender sensitivity english, high low game, identify math theorems, intersect geometry, muslim violence bias, persian idioms, protein interacting sites, scientiﬁc press release, self evaluation courtroom, social support, spelling bee, taboo, training on test set, truthful qa, yes no black white, dark humor detection, dyck languages, moral permissibility, ruin names Model gets worse with scale: bbq lite, bias from probabilities, diverse social bias, movie recommendation, unqover Not enough examples: known unknowns, suicide risk, what is the tao Incomplete evals: convinceme, long context integration, medical questions russian Other: arithmetic (emergent at 1B, which is none of the above categories), few-shot nlg (not sure why BLEURT is negative here) F PaLM 62B is emergent but GPT-3 and LaMDA are not We made the point in §5.2 that scale is not the only factor in emergence, since PaLM 62B shows emergence on many BIG-Bench tasks for which GPT-3 175B and LaMDA 137B do not, even though PaLM 62B has fewer model parameter and less training FLOPs. This is the list of tasks: anachronisms, ascii word recognition, conceptual combinations, cryptonite, disam- biguation qa, emoji movie, goal step wikihow, gre reading comprehension, linguistics puzzles, logic grid puzzle, metaphor boolean, metaphor understanding, odd one out, parsinlu qa.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tune As You Scale: Hyperparameter Optimization For Compute Efficient Training : Hyperparameter tuning of deep learning models can lead to order-of-magnitude performance gains for the same amount of compute. Despite this, sys- tematic tuning is uncommon, particularly for large models, which are expensive to evaluate and tend to have many hyperparameters, necessitating dif- ficult judgment calls about tradeoffs, budgets, and search bounds. To address these issues and pro- pose a practical method for robustly tuning large models, we present Cost-Aware Pareto Region Bayesian Search (CARBS), a Bayesian optimiza- tion algorithm that performs local search around the performance-cost Pareto frontier. CARBS does well even in unbounded search spaces with many hyperparameters, learns scaling relation- ships so that it can tune models even as they are scaled up, and automates much of the “black magic” of tuning. Among our results, we effec- tively solve the entire ProcGen benchmark just by tuning a simple baseline (PPO, as provided in the original ProcGen paper). We also reproduce the model size vs. training tokens scaling result from the Chinchilla project (Hoffmann et al., 2022), while simultaneously discovering scaling laws for every other hyperparameter, via an easy auto- mated process that uses significantly less compute and is applicable to any deep learning problem (not just language models).'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['text'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "lines = extract_text_from_pdf(pdf_path).splitlines()\n",
    "lines = [line for line in lines if len(line) >= 4]\n",
    "for i in range(len(lines)):\n",
    "    if lines[i].lower() == 'abstract':\n",
    "        abstract_idx = i + 1\n",
    "    if lines[i].lower() == 'introduction':\n",
    "        intro_idx = i\n",
    "        abstract = \" \".join(lines[abstract_idx:intro_idx])\n",
    "        text =  \" \".join(lines[intro_idx:])\n",
    "    #return abstract, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Introduction Language models (LMs), such as GPT-2 (Radford et al., 2018, 2019a), BERT (Devlin et al., 2018), T5 (Raffel et al., 2020), or GPT-3 (Brown et al., 2020), exhibit a remarkable ability to ca'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pdf_lines(pdf_path, str_len):\n",
    "    # remove  lines with string length < str_len\n",
    "    # remove lines before theabstract\n",
    "    # return lines\n",
    "    lines = extract_text_from_pdf(pdf_path).splitlines()\n",
    "    lines = [line for line in lines if len(line) >= str_len]\n",
    "    for i in range(len(lines)):\n",
    "        if lines[i].lower() == 'abstract':\n",
    "            return \" \".join(lines[i:])\n",
    "    warnings.warn(\"Warning: Did not parse abstract of paper\", pdf_path)\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Created train_corpus.txt and val_corpus.txt\n",
      "\n",
      " Created train and val splits\n"
     ]
    }
   ],
   "source": [
    "train, val = create_dataset(dir_path, arxiv_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "\n",
    "def tokenize(dataset):\n",
    "     \n",
    "    split =  dataset.split\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    prefix = \"abstract: \"\n",
    "    def process(example):\n",
    "        #based both on karpathy's prepare.py and  https://huggingface.co/docs/transformers/tasks/summarization\n",
    "        example_input = [prefix + doc for doc in example[\"text\"]]\n",
    "        ids = enc.encode_ordinary(example_input) # encode_ordinary ignores any special tokens\n",
    "        ids.append(enc.eot_token) # add the end of text token, e.g. 50256 for gpt2 bpe\n",
    "        # note: check if eot should be prepended not appended \n",
    "        label = enc.encode_ordinary(example[\"abstract\"])\n",
    "        #out = {'ids': ids, 'len': len(ids)}\n",
    "        return {'ids': ids, 'label': label, 'len': len(ids)}\n",
    "\n",
    "    tokenized = dataset.map( #https://github.com/huggingface/datasets/blob/src/datasets/dataset_dict.py\n",
    "            process,\n",
    "            remove_columns=['text'],\n",
    "            desc=\"tokenizing the splits\"\n",
    "        )\n",
    "\n",
    "    for split, dset in tokenized.items():\n",
    "            arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
    "            filename = os.path.join(os.path.dirname(__file__), f'{split}.bin')\n",
    "            dtype = np.uint16 # (can do since enc.max_token_value == 50256 is < 2**16)\n",
    "            arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "            total_batches = 1024\n",
    "\n",
    "            idx = 0\n",
    "            for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
    "                # Batch together samples for faster write\n",
    "                batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
    "                arr_batch = np.concatenate(batch['ids'])\n",
    "                # Write into mmap\n",
    "                arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "                idx += len(arr_batch)\n",
    "            arr.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"abstract: \"\n",
    "def preprocess_function(examples):\n",
    "    #based on https://huggingface.co/docs/transformers/tasks/summarization\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"abstract\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_billsum = train_split.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove references, use parts of the article to predict other parts, e.g. predict the abstract and conclusion\n",
    "\n",
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from py_pdf_parser.loaders import load_file\n",
    "\n",
    "\n",
    "import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "#import sys\n",
    "#print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import bibtexparser\n",
    "\n",
    "dir_path = \"papers\"\n",
    "\n",
    "arxiv_id = \"2210.03162\"\n",
    "arxiv_id_bib = \"2210-03162\"\n",
    "pdf_url = f\"https://export.arxiv.org/pdf/{arxiv_id}\"\n",
    "bibtex_url = f\"https://dblp.uni-trier.de/rec/journals/corr/abs-{arxiv_id_bib}.bib?param=1\"\n",
    "\n",
    "pdf_local_path = os.path.join(dir_path, \"test.pdf\")\n",
    "bibtex_local_path = os.path.join(dir_path, \"test.bib\")\n",
    "\n",
    "if pdf_url is not None:\n",
    "    pdf_path, headers = urlretrieve(pdf_url, pdf_local_path)\n",
    "\n",
    "if bibtex_url is not None:\n",
    "    bib_path, headers = urlretrieve(bibtex_url, bibtex_local_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_bibtex(arxiv_id, dir_path):\n",
    "\n",
    "    arxiv_id_bib = str(arxiv_id).replace('.', '-')\n",
    "    bibtex_url = f\"https://dblp.uni-trier.de/rec/journals/corr/abs-{arxiv_id_bib}.bib?param=1\"\n",
    "\n",
    "    bibtex_local_path = os.path.join(dir_path, f\"{arxiv_id_bib}.bib\")\n",
    "\n",
    "    if bibtex_url is not None:\n",
    "        bib_path, _ = urlretrieve(bibtex_url, bibtex_local_path)\n",
    "\n",
    "    return bib_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_paper(arxiv_id, dir_path):\n",
    "\n",
    "    pdf_url = f\"https://export.arxiv.org/pdf/{arxiv_id}\"\n",
    "\n",
    "    #arxiv_id =  str(arxiv_id).replace('.', '-')\n",
    "    pdf_local_path = os.path.join(dir_path, f\"{arxiv_id}.pdf\")\n",
    "\n",
    "    if pdf_url is not None:\n",
    "        pdf_path, _ = urlretrieve(pdf_url, pdf_local_path)\n",
    "\n",
    "    return pdf_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'papers/2210.03162.pdf'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_paper(2210.03162, dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models',\n",
       " ['David Wingate and', ' Mohammad Shoeybi and', ' Taylor Sorensen'])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_metadata(bib_path):\n",
    "    library = bibtexparser.parse_file(bib_path) \n",
    "\n",
    "    authors = library.entries[0].fields_dict['author']._value.splitlines()\n",
    "    authors = [re.sub(\"\\s{2,}\",\" \",elem.replace('and ','')) for elem in authors] \n",
    "\n",
    "    title = library.entries[0].fields_dict['title']._value\n",
    "    title = re.sub(\"\\s{2,}\",\" \",title)\n",
    "    return title, authors\n",
    "\n",
    "extract_metadata(bib_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prompt Compression and Contrastive Conditioning for Controllability                  and Toxicity Reduction in Language Models'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title.replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "path, headers = urlretrieve(url, filename)\n",
    "for name, value in headers.items():\n",
    "    print(name, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path =  \"papers\"\n",
    "\n",
    "def list_pdfs(dir_path):\n",
    "     pathlist = Path(dir_path).glob('**/*.pdf')\n",
    "     titles = []\n",
    "     for path in pathlist:\n",
    "          titles.append(str(path))\n",
    "\n",
    "     return titles\n",
    "\n",
    "def extract_text(file_path):\n",
    "     return extract_text(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['papers/Emergent_Abilities_of_Large_Language_Models.pdf',\n",
       " 'papers/Prompt_Programming_for_Large_Language_Models_Beyond_the_Few-Shot_Paradigm.pdf']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_pdfs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text('papers/Emergent_Abilities_of_Large_Language_Models.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '8', '6', '7', '0', '.', '6', '0', '2', '2', ':', 'v', 'i', 'X', 'r', 'a', '', 'Published in Transactions on Machine Learning Research (08/2022)', '', 'Emergent Abilities of Large Language Models', '', 'Jason Wei 1', 'Yi Tay 1', 'Rishi Bommasani 2', 'Colin Raﬀel 3', 'Barret Zoph 1', 'Sebastian Borgeaud 4', 'Dani Yogatama 4', 'Maarten Bosma 1', 'Denny Zhou 1', 'Donald Metzler 1', 'Ed H. Chi 1', 'Tatsu']\n"
     ]
    }
   ],
   "source": [
    "print(text[40:350].splitlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "\n",
    "output_string = StringIO()\n",
    "with open('papers/Emergent_Abilities_of_Large_Language_Models.pdf', 'rb') as in_file:\n",
    "    parser = PDFParser(in_file)\n",
    "    doc = PDFDocument(parser)\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    for page in PDFPage.create_pages(doc):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "print(output_string.getvalue()[40:80] == text[40:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['papers/Emergent_Abilities_of_Large_Language_Models.pdf',\n",
       " 'papers/Prompt_Programming_for_Large_Language_Models_Beyond_the_Few-Shot_Paradigm.pdf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_pdfs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "document =  load_pdf('papers/Emergent_Abilities_of_Large_Language_Models.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<py_pdf_parser.components.PDFDocument at 0x7fe905455b40>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PDFElement tags: set(), font: 'YBSOMS+LMRoman10-Regular,10.0'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract = document.elements.filter_by_text_equal(\"2\").extract_single_element()\n",
    "abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoElementFoundError",
     "evalue": "There are no elements in the ElementList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoElementFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/nanoGPT/finetune/finetune_on_papers.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f70726f6a65637473227d/home/nanoGPT/finetune/finetune_on_papers.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m document\u001b[39m.\u001b[39;49melements\u001b[39m.\u001b[39;49mto_the_right_of(abstract)\u001b[39m.\u001b[39;49mextract_single_element()\u001b[39m.\u001b[39mtext()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py_pdf_parser/filtering.py:783\u001b[0m, in \u001b[0;36mElementList.extract_single_element\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[39mReturns only element in the ElementList, provided there is only one element.\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[39m    PDFElement: The single element remaining in the list.\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindexes) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 783\u001b[0m     \u001b[39mraise\u001b[39;00m NoElementFoundError(\u001b[39m\"\u001b[39m\u001b[39mThere are no elements in the ElementList\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    784\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindexes) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    785\u001b[0m     \u001b[39mraise\u001b[39;00m MultipleElementsFoundError(\n\u001b[1;32m    786\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThere are \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindexes)\u001b[39m}\u001b[39;00m\u001b[39m elements in the ElementList\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    787\u001b[0m     )\n",
      "\u001b[0;31mNoElementFoundError\u001b[0m: There are no elements in the ElementList"
     ]
    }
   ],
   "source": [
    "document.elements.to_the_right_of(abstract).extract_single_element().text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
