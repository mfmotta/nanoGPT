{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from pathlib import Path\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"papers\"\n",
    "arxiv_ids = [2205.14135, 1904.07094, 2203.00759, 2104.08691, 2212.01681, 2210.03162, 2206.07682, 2306.08055] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_file_path, abstract_file_path = create_corpus(dir_path, arxiv_ids)\n",
    "text_file_path, abstract_file_path ='data/papers_text.txt', 'data/papers_abstract.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'abstract'],\n",
       "        num_rows: 6\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'abstract'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = create_dataset(text_file_path, abstract_file_path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing the splits: 100%|██████████| 6/6 [00:00<00:00, 88.72 examples/s]\n",
      "tokenizing the splits: 100%|██████████| 2/2 [00:00<00:00, 76.43 examples/s]\n",
      "writing data/train.bin: 100%|██████████| 2/2 [00:00<00:00, 468.45it/s]\n",
      "writing data/val.bin: 100%|██████████| 2/2 [00:00<00:00, 700.86it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = tokenize(dataset)\n",
    "create_sharded_dataset(tokenized_dataset, \"data\", total_batches =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove references, use parts of the article to predict other parts, e.g. predict the abstract and conclusion\n",
    "\n",
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from py_pdf_parser.loaders import load_file\n",
    "\n",
    "\n",
    "import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "#import sys\n",
    "#print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import bibtexparser\n",
    "\n",
    "dir_path = \"papers\"\n",
    "\n",
    "arxiv_id = \"2210.03162\"\n",
    "arxiv_id_bib = \"2210-03162\"\n",
    "pdf_url = f\"https://export.arxiv.org/pdf/{arxiv_id}\"\n",
    "bibtex_url = f\"https://dblp.uni-trier.de/rec/journals/corr/abs-{arxiv_id_bib}.bib?param=1\"\n",
    "\n",
    "pdf_local_path = os.path.join(dir_path, \"test.pdf\")\n",
    "bibtex_local_path = os.path.join(dir_path, \"test.bib\")\n",
    "\n",
    "if pdf_url is not None:\n",
    "    pdf_path, headers = urlretrieve(pdf_url, pdf_local_path)\n",
    "\n",
    "if bibtex_url is not None:\n",
    "    bib_path, headers = urlretrieve(bibtex_url, bibtex_local_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_bibtex(arxiv_id, dir_path):\n",
    "\n",
    "    arxiv_id_bib = str(arxiv_id).replace('.', '-')\n",
    "    bibtex_url = f\"https://dblp.uni-trier.de/rec/journals/corr/abs-{arxiv_id_bib}.bib?param=1\"\n",
    "\n",
    "    bibtex_local_path = os.path.join(dir_path, f\"{arxiv_id_bib}.bib\")\n",
    "\n",
    "    if bibtex_url is not None:\n",
    "        bib_path, _ = urlretrieve(bibtex_url, bibtex_local_path)\n",
    "\n",
    "    return bib_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_paper(arxiv_id, dir_path):\n",
    "\n",
    "    pdf_url = f\"https://export.arxiv.org/pdf/{arxiv_id}\"\n",
    "\n",
    "    #arxiv_id =  str(arxiv_id).replace('.', '-')\n",
    "    pdf_local_path = os.path.join(dir_path, f\"{arxiv_id}.pdf\")\n",
    "\n",
    "    if pdf_url is not None:\n",
    "        pdf_path, _ = urlretrieve(pdf_url, pdf_local_path)\n",
    "\n",
    "    return pdf_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'papers/2210.03162.pdf'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_paper(2210.03162, dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models',\n",
       " ['David Wingate and', ' Mohammad Shoeybi and', ' Taylor Sorensen'])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_metadata(bib_path):\n",
    "    library = bibtexparser.parse_file(bib_path) \n",
    "\n",
    "    authors = library.entries[0].fields_dict['author']._value.splitlines()\n",
    "    authors = [re.sub(\"\\s{2,}\",\" \",elem.replace('and ','')) for elem in authors] \n",
    "\n",
    "    title = library.entries[0].fields_dict['title']._value\n",
    "    title = re.sub(\"\\s{2,}\",\" \",title)\n",
    "    return title, authors\n",
    "\n",
    "extract_metadata(bib_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prompt Compression and Contrastive Conditioning for Controllability                  and Toxicity Reduction in Language Models'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title.replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "path, headers = urlretrieve(url, filename)\n",
    "for name, value in headers.items():\n",
    "    print(name, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path =  \"papers\"\n",
    "\n",
    "def list_pdfs(dir_path):\n",
    "     pathlist = Path(dir_path).glob('**/*.pdf')\n",
    "     titles = []\n",
    "     for path in pathlist:\n",
    "          titles.append(str(path))\n",
    "\n",
    "     return titles\n",
    "\n",
    "def extract_text(file_path):\n",
    "     return extract_text(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['papers/Emergent_Abilities_of_Large_Language_Models.pdf',\n",
       " 'papers/Prompt_Programming_for_Large_Language_Models_Beyond_the_Few-Shot_Paradigm.pdf']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_pdfs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text('papers/Emergent_Abilities_of_Large_Language_Models.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '8', '6', '7', '0', '.', '6', '0', '2', '2', ':', 'v', 'i', 'X', 'r', 'a', '', 'Published in Transactions on Machine Learning Research (08/2022)', '', 'Emergent Abilities of Large Language Models', '', 'Jason Wei 1', 'Yi Tay 1', 'Rishi Bommasani 2', 'Colin Raﬀel 3', 'Barret Zoph 1', 'Sebastian Borgeaud 4', 'Dani Yogatama 4', 'Maarten Bosma 1', 'Denny Zhou 1', 'Donald Metzler 1', 'Ed H. Chi 1', 'Tatsu']\n"
     ]
    }
   ],
   "source": [
    "print(text[40:350].splitlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "\n",
    "output_string = StringIO()\n",
    "with open('papers/Emergent_Abilities_of_Large_Language_Models.pdf', 'rb') as in_file:\n",
    "    parser = PDFParser(in_file)\n",
    "    doc = PDFDocument(parser)\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    for page in PDFPage.create_pages(doc):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "print(output_string.getvalue()[40:80] == text[40:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['papers/Emergent_Abilities_of_Large_Language_Models.pdf',\n",
       " 'papers/Prompt_Programming_for_Large_Language_Models_Beyond_the_Few-Shot_Paradigm.pdf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_pdfs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "document =  load_pdf('papers/Emergent_Abilities_of_Large_Language_Models.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<py_pdf_parser.components.PDFDocument at 0x7fe905455b40>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PDFElement tags: set(), font: 'YBSOMS+LMRoman10-Regular,10.0'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract = document.elements.filter_by_text_equal(\"2\").extract_single_element()\n",
    "abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoElementFoundError",
     "evalue": "There are no elements in the ElementList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoElementFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/nanoGPT/finetune/finetune_on_papers.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f70726f6a65637473227d/home/nanoGPT/finetune/finetune_on_papers.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m document\u001b[39m.\u001b[39;49melements\u001b[39m.\u001b[39;49mto_the_right_of(abstract)\u001b[39m.\u001b[39;49mextract_single_element()\u001b[39m.\u001b[39mtext()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py_pdf_parser/filtering.py:783\u001b[0m, in \u001b[0;36mElementList.extract_single_element\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[39mReturns only element in the ElementList, provided there is only one element.\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[39m    PDFElement: The single element remaining in the list.\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindexes) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 783\u001b[0m     \u001b[39mraise\u001b[39;00m NoElementFoundError(\u001b[39m\"\u001b[39m\u001b[39mThere are no elements in the ElementList\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    784\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindexes) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    785\u001b[0m     \u001b[39mraise\u001b[39;00m MultipleElementsFoundError(\n\u001b[1;32m    786\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThere are \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindexes)\u001b[39m}\u001b[39;00m\u001b[39m elements in the ElementList\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    787\u001b[0m     )\n",
      "\u001b[0;31mNoElementFoundError\u001b[0m: There are no elements in the ElementList"
     ]
    }
   ],
   "source": [
    "document.elements.to_the_right_of(abstract).extract_single_element().text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
